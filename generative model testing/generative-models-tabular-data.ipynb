{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:00.146371Z",
     "iopub.status.busy": "2026-02-21T21:14:00.145996Z",
     "iopub.status.idle": "2026-02-21T21:14:04.735084Z",
     "shell.execute_reply": "2026-02-21T21:14:04.733917Z",
     "shell.execute_reply.started": "2026-02-21T21:14:00.146339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.42.54 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet torch torchvision pandas numpy scikit-learn matplotlib tqdm sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.737821Z",
     "iopub.status.busy": "2026-02-21T21:14:04.737427Z",
     "iopub.status.idle": "2026-02-21T21:14:04.743982Z",
     "shell.execute_reply": "2026-02-21T21:14:04.742806Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.737785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer, CopulaGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.746435Z",
     "iopub.status.busy": "2026-02-21T21:14:04.745502Z",
     "iopub.status.idle": "2026-02-21T21:14:04.763972Z",
     "shell.execute_reply": "2026-02-21T21:14:04.763115Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.746388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#PREPROCESSING\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#GENERATIVE MODELS\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer, CopulaGANSynthesizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "A generative model is a machine learning model designed to create new data that is similar to its training data. Generative artificial intelligence (AI) models learn the patterns and distributions of the training data, then apply those understandings to generate novel content in response to new input data. (https://www.ibm.com/think/topics/generative-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 3 Big Categories of Generative Models\n",
    "## Generative Adversarial Networks (GANs)\n",
    "\n",
    "A generative adversarial network, or GAN, is a machine learning model designed to generate realistic data by learning patterns from existing training datasets. It operates within an unsupervised learning framework by using deep learning techniques, where two neural networks work in opposition—one generates data, while the other evaluates whether the data is real or generated. (https://www.ibm.com/think/topics/generative-adversarial-networks)\n",
    "\n",
    "\n",
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "Variational autoencoders (VAEs) are generative models used in machine learning (ML) to generate new data in the form of variations of the input data they’re trained on. In addition to this, they also perform tasks common to other autoencoders, such as denoising. (https://www.ibm.com/think/topics/variational-autoencoder)\n",
    "\n",
    "## Diffusion Models\n",
    "\n",
    "Diffusion models are generative models used primarily for image generation and other computer vision tasks. Diffusion-based neural networks are trained through deep learning to progressively “diffuse” samples with random noise, then reverse that diffusion process. (https://www.ibm.com/think/topics/diffusion-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM\n",
    "Denoising Diffusion Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.766702Z",
     "iopub.status.busy": "2026-02-21T21:14:04.766310Z",
     "iopub.status.idle": "2026-02-21T21:14:04.793168Z",
     "shell.execute_reply": "2026-02-21T21:14:04.792169Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.766672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DiffusionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.input_embed = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.block2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.block3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        \n",
    "        self.final = nn.Linear(hidden_dim, input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_embed(t.float().view(-1, 1))\n",
    "        x_emb = self.input_embed(x)\n",
    "        h = self.activation(x_emb + t_emb)\n",
    "        \n",
    "        h = h + self.block1(h)\n",
    "        h = h + self.block2(h)\n",
    "        h = h + self.block3(h)\n",
    "        return self.final(h)\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, input_dim, n_steps=100, device='cpu'):\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        self.model = DiffusionNetwork(input_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.beta = torch.linspace(1e-4, 0.02, n_steps).to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def train_one_batch(self, x0):\n",
    "        self.model.train()\n",
    "        \n",
    "        t = torch.randint(0, self.n_steps, (x0.shape[0],), device=self.device).long()\n",
    "        \n",
    "        epsilon = torch.randn_like(x0)\n",
    "        \n",
    "        sqrt_alpha = torch.sqrt(self.alpha_hat[t]).view(-1, 1)\n",
    "        sqrt_one_minus = torch.sqrt(1 - self.alpha_hat[t]).view(-1, 1)\n",
    "        x_t = sqrt_alpha * x0 + sqrt_one_minus * epsilon\n",
    "        \n",
    "        pred_epsilon = self.model(x_t, t.float() / self.n_steps)\n",
    "        \n",
    "        loss = self.criterion(pred_epsilon, epsilon)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples):\n",
    "        self.model.eval()\n",
    "        x = torch.randn((n_samples, self.model.final.out_features)).to(self.device)\n",
    "        for i in reversed(range(self.n_steps)):\n",
    "            t = torch.tensor([i] * n_samples, device=self.device)\n",
    "            pred_eps = self.model(x, t.float() / self.n_steps)\n",
    "            alpha_t, alpha_hat_t, beta_t = self.alpha[i], self.alpha_hat[i], self.beta[i]\n",
    "            \n",
    "            noise = torch.randn_like(x) if i > 0 else torch.zeros_like(x)\n",
    "            term1 = 1 / torch.sqrt(alpha_t)\n",
    "            term2 = (1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)\n",
    "            x = term1 * (x - term2 * pred_eps) + torch.sqrt(beta_t) * noise\n",
    "        return x\n",
    "\n",
    "def get_synthetic_data_ddpm(X_in, y_in, n_samples_ratio=0.5, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray()\n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "    \n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "    \n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "    \n",
    "    internal_scaler = StandardScaler()\n",
    "    data_scaled = internal_scaler.fit_transform(data_combined)\n",
    "    \n",
    "    dataset = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    ddpm = DiffusionModel(input_dim=dataset.shape[1], n_steps=50, device=device)\n",
    "    \n",
    "    batch_size = min(256, len(dataset))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            ddpm.train_one_batch(batch)\n",
    "            \n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "    generated_scaled = ddpm.sample(n_samples).cpu().numpy()\n",
    "    \n",
    "    generated_data = internal_scaler.inverse_transform(generated_scaled)\n",
    "    \n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "    \n",
    "    return X_syn, y_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.795363Z",
     "iopub.status.busy": "2026-02-21T21:14:04.794382Z",
     "iopub.status.idle": "2026-02-21T21:14:04.817292Z",
     "shell.execute_reply": "2026-02-21T21:14:04.816476Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.795317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math, random, time\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_synthetic_data_gan(X_in, y_in, n_samples_ratio=0.5, epochs=100, latent_dim=64):\n",
    "    \"\"\"\n",
    "    Train a GAN on X_in, y_in and return synthetic X_syn, y_syn.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray()\n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "    \n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "    \n",
    "    internal_scaler = StandardScaler()\n",
    "    data_scaled = internal_scaler.fit_transform(data_combined)\n",
    "    \n",
    "    real_data = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    data_dim = real_data.shape[1]\n",
    "    \n",
    "    generator = Generator(latent_dim, data_dim).to(device)\n",
    "    discriminator = Discriminator(data_dim).to(device)\n",
    "    \n",
    "    lr = 0.0002\n",
    "    opt_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    batch_size = min(128, len(real_data))\n",
    "    n_batches = int(np.ceil(len(real_data) / batch_size))\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(real_data))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            idx = indices[i*batch_size : (i+1)*batch_size]\n",
    "            real_batch = real_data[idx]\n",
    "            curr_batch_size = real_batch.size(0)\n",
    "            \n",
    "            real_labels = torch.ones(curr_batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(curr_batch_size, 1).to(device)\n",
    "            \n",
    "            opt_d.zero_grad()\n",
    "            \n",
    "            outputs = discriminator(real_batch)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            \n",
    "            z = torch.randn(curr_batch_size, latent_dim).to(device)\n",
    "            fake_batch = generator(z)\n",
    "            outputs = discriminator(fake_batch.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "            opt_g.zero_grad()\n",
    "            \n",
    "            outputs = discriminator(fake_batch)\n",
    "            g_loss = criterion(outputs, real_labels) \n",
    "            \n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "    generator.eval()\n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim).to(device)\n",
    "        generated_scaled = generator(z).cpu().numpy()\n",
    "\n",
    "    generated_data = internal_scaler.inverse_transform(generated_scaled)\n",
    "    \n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "    \n",
    "    return X_syn, y_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.818835Z",
     "iopub.status.busy": "2026-02-21T21:14:04.818497Z",
     "iopub.status.idle": "2026-02-21T21:14:04.839564Z",
     "shell.execute_reply": "2026-02-21T21:14:04.838730Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.818796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.act(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.act(self.fc2(z))\n",
    "        return self.fc3(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar):\n",
    "    recon_loss = nn.MSELoss()(x_hat, x)\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl\n",
    "\n",
    "def get_synthetic_data_vae(\n",
    "    X_in, y_in,\n",
    "    n_samples_ratio=0.5,\n",
    "    epochs=80,\n",
    "    latent_dim=16,\n",
    "    hidden_dim=128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a VAE on the joint (X, y) distribution and return synthetic samples.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray()\n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "\n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "\n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_combined)\n",
    "\n",
    "    data_t = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    input_dim = data_t.shape[1]\n",
    "\n",
    "    model = VAE(input_dim, latent_dim, hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    batch_size = min(256, len(data_t))\n",
    "    loader = torch.utils.data.DataLoader(data_t, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch in loader:\n",
    "            x_hat, mu, logvar = model(batch)\n",
    "            loss = vae_loss(batch, x_hat, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim).to(device)\n",
    "        generated_scaled = model.decode(z).cpu().numpy()\n",
    "\n",
    "    generated_data = scaler.inverse_transform(generated_scaled)\n",
    "\n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "\n",
    "    return X_syn, y_syn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDV (Synthetic Data Vault - library)\n",
    "- CTGAN\n",
    "- TVAE\n",
    "- CopulaGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.840973Z",
     "iopub.status.busy": "2026-02-21T21:14:04.840700Z",
     "iopub.status.idle": "2026-02-21T21:14:04.855026Z",
     "shell.execute_reply": "2026-02-21T21:14:04.854062Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.840947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_synthetic_data_sdv(X_in, y_in, model_name=\"ctgan\", n_samples_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Wraps SDV models to work with Numpy arrays/Sparse matrices.\n",
    "    model_name options: 'ctgan', 'tvae', 'copulagan'\n",
    "    \"\"\"\n",
    "    \n",
    "    if hasattr(X_in, \"toarray\"):\n",
    "        X_in = X_in.toarray()\n",
    "\n",
    "    X_arr = X_in.values if hasattr(X_in, \"values\") else X_in\n",
    "    y_arr = y_in.values if hasattr(y_in, \"values\") else y_in\n",
    "    y_arr = y_arr.reshape(-1, 1)\n",
    "    \n",
    "    n_features = X_arr.shape[1]\n",
    "    col_names = [f\"col_{i}\" for i in range(n_features)] + [\"target\"]\n",
    "    \n",
    "    data_combined = np.hstack([X_arr, y_arr])\n",
    "    df_temp = pd.DataFrame(data_combined, columns=col_names)\n",
    "\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=df_temp)\n",
    "\n",
    "    is_using_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    if model_name.lower() == \"tvae\":\n",
    "        model = TVAESynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif model_name.lower() == \"copulagan\":\n",
    "        model = CopulaGANSynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif model_name.lower() == \"ctgan\":\n",
    "        model = CTGANSynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    model.fit(df_temp)\n",
    "    \n",
    "    n_samples = int(len(X_arr) * n_samples_ratio)\n",
    "    synthetic_df = model.sample(num_rows=n_samples)\n",
    "    \n",
    "    data_syn = synthetic_df.values\n",
    "    \n",
    "    X_syn = data_syn[:, :-1]\n",
    "    y_syn = data_syn[:, -1]\n",
    "\n",
    "    return X_syn, y_syn\n",
    "\n",
    "def get_synthetic_data_ctgan(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"ctgan\", n_samples_ratio=n_samples_ratio)\n",
    "\n",
    "def get_synthetic_data_tvae(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"tvae\", n_samples_ratio=n_samples_ratio)\n",
    "\n",
    "def get_synthetic_data_copulagan(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"copulagan\", n_samples_ratio=n_samples_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.856455Z",
     "iopub.status.busy": "2026-02-21T21:14:04.856124Z",
     "iopub.status.idle": "2026-02-21T21:14:04.874797Z",
     "shell.execute_reply": "2026-02-21T21:14:04.873508Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.856426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_samples_ratio = 1.00\n",
    "gen_model_params = {\n",
    "    \"DDPM\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\": 100\n",
    "    },\n",
    "    \"GAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\": 100, \n",
    "        \"latent_dim\": 64\n",
    "    },\n",
    "    \"VAE\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\":80,\n",
    "        \"latent_dim\":16,\n",
    "        \"hidden_dim\":128,\n",
    "    },\n",
    "    \"CTGAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    },\n",
    "    \"TVAE\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    },\n",
    "    \"COPULAGAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.876425Z",
     "iopub.status.busy": "2026-02-21T21:14:04.876082Z",
     "iopub.status.idle": "2026-02-21T21:14:04.891851Z",
     "shell.execute_reply": "2026-02-21T21:14:04.890707Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.876397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def augment_data(model_name, X_train, y_train, params={}):\n",
    "    \"\"\"\n",
    "    model_name options: DDPM, GAN, VAE, CTGAN, TVAE, COPULAGAN\n",
    "    \"\"\"\n",
    "    if model_name.upper() == \"DDPM\":\n",
    "        return get_synthetic_data_ddpm(X_train, y_train, **params)\n",
    "    elif model_name.upper() == \"GAN\":\n",
    "        return get_synthetic_data_gan(X_train, y_train, **params)\n",
    "    elif model_name.upper() == \"VAE\":\n",
    "        return get_synthetic_data_vae(X_train, y_train, **params)\n",
    "    elif model_name.upper() == \"COPULAGAN\":\n",
    "        return get_synthetic_data_copulagan(X_train, y_train, **params)\n",
    "    elif model_name.upper() == \"TVAE\":\n",
    "        return get_synthetic_data_tvae(X_train, y_train, **params)\n",
    "    else: # Defaults to CTGAN\n",
    "        return get_synthetic_data_ctgan(X_train, y_train, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Testar os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.895224Z",
     "iopub.status.busy": "2026-02-21T21:14:04.894802Z",
     "iopub.status.idle": "2026-02-21T21:14:04.924347Z",
     "shell.execute_reply": "2026-02-21T21:14:04.923311Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.895195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loaded_dataset = \"california\"\n",
    "\n",
    "if loaded_dataset == \"california\":\n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    data = fetch_california_housing()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "elif loaded_dataset == \"custo\":\n",
    "    import kagglehub\n",
    "    from kagglehub import KaggleDatasetAdapter\n",
    "    \n",
    "    file_path = \"dataset.csv\"\n",
    "    dataset_path = \"username/dataset\"\n",
    "\n",
    "    df = kagglehub.load_dataset(\n",
    "      KaggleDatasetAdapter.PANDAS,\n",
    "      dataset_path,\n",
    "      file_path,\n",
    "    )\n",
    "    df = df.rename(columns={'custo': 'Target'})\n",
    "    \n",
    "    X = df.drop(columns={\"Target\"})\n",
    "    y = df[\"Target\"]\n",
    "\n",
    "    cat_cols = [c for c in X.columns if X[c].dtype == 'object' or str(X[c].dtype).startswith('category')]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "    \n",
    "    ord_cols = []\n",
    "    \n",
    "    if \"class_etaria\" in cat_cols:\n",
    "        ord_cols.append(\"class_etaria\")\n",
    "        cat_cols.remove(\"class_etaria\")\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    ordinal_encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "    \n",
    "    preprocess_transformer = ColumnTransformer(\n",
    "       transformers=[(\"onehot_encoder\", onehot_encoder, cat_cols),\n",
    "                     (\"ordinal_encoder\", ordinal_encoder, ord_cols)],\n",
    "       remainder=\"passthrough\"\n",
    "    )\n",
    "\n",
    "    X = preprocess_transformer.fit_transform(X)\n",
    "    y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:14:04.926123Z",
     "iopub.status.busy": "2026-02-21T21:14:04.925585Z",
     "iopub.status.idle": "2026-02-21T21:51:18.515612Z",
     "shell.execute_reply": "2026-02-21T21:51:18.514643Z",
     "shell.execute_reply.started": "2026-02-21T21:14:04.926077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "synth_data = {}\n",
    "\n",
    "print(\"GAN\")\n",
    "X_aug, y_aug = augment_data(\"GAN\", X, y, gen_model_params[\"GAN\"])\n",
    "synth_data[\"GAN\"] = {\"x\": X_aug, \"y\": y_aug}\n",
    "\n",
    "print(\"CTGAN\")\n",
    "X_aug, y_aug = augment_data(\"CTGAN\", X, y, gen_model_params[\"CTGAN\"])\n",
    "synth_data[\"CTGAN\"] = {\"x\": X_aug, \"y\": y_aug}\n",
    "\n",
    "print(\"COPULAGAN\")\n",
    "X_aug, y_aug = augment_data(\"COPULAGAN\", X, y, gen_model_params[\"COPULAGAN\"])\n",
    "synth_data[\"COPULAGAN\"] = {\"x\": X_aug, \"y\": y_aug}\n",
    "\n",
    "print(\"VAE\")\n",
    "X_aug, y_aug = augment_data(\"VAE\", X, y, gen_model_params[\"VAE\"])\n",
    "synth_data[\"VAE\"] = {\"x\": X_aug, \"y\": y_aug}\n",
    "\n",
    "print(\"TVAE\")\n",
    "X_aug, y_aug = augment_data(\"TVAE\", X, y, gen_model_params[\"TVAE\"])\n",
    "synth_data[\"TVAE\"] = {\"x\": X_aug, \"y\": y_aug}\n",
    "\n",
    "print(\"DDPM\")\n",
    "X_aug, y_aug = augment_data(\"DDPM\", X, y, gen_model_params[\"DDPM\"])\n",
    "synth_data[\"DDPM\"] = {\"x\": X_aug, \"y\": y_aug}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:51:18.517296Z",
     "iopub.status.busy": "2026-02-21T21:51:18.516939Z",
     "iopub.status.idle": "2026-02-21T21:51:22.202338Z",
     "shell.execute_reply": "2026-02-21T21:51:22.201427Z",
     "shell.execute_reply.started": "2026-02-21T21:51:18.517259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_features = X.shape[1]\n",
    "\n",
    "for i in range(n_features):\n",
    "    plt.hist(X[:, i], bins=30, alpha=0.4, density=True, label=\"Original\")\n",
    "    plt.hist(synth_data[\"DDPM\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"DDPM\")\n",
    "    plt.hist(synth_data[\"GAN\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"GAN\")\n",
    "    plt.hist(synth_data[\"VAE\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"VAE\")\n",
    "    plt.hist(synth_data[\"COPULAGAN\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"COPULAGAN\")\n",
    "    plt.hist(synth_data[\"TVAE\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"TVAE\")\n",
    "    plt.hist(synth_data[\"CTGAN\"][\"x\"][:, i], bins=30, alpha=0.4, density=True, label=\"CTGAN\")\n",
    "    plt.title(f\"Feature {i}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:51:22.203856Z",
     "iopub.status.busy": "2026-02-21T21:51:22.203569Z",
     "iopub.status.idle": "2026-02-21T21:51:22.219171Z",
     "shell.execute_reply": "2026-02-21T21:51:22.218327Z",
     "shell.execute_reply.started": "2026-02-21T21:51:22.203827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"/kaggle/working/synth data {loaded_dataset} {n_samples_ratio}.pkl\", \"wb\") as f:\n",
    "    print(synth_data.keys())\n",
    "    pickle.dump(synth_data, f)\n",
    "\n",
    "#with open(\"/kaggle/working/synthetic_data_dict.pkl\", \"rb\") as f:\n",
    "#    synth_data = pickle.load(f)\n",
    "#    print(synth_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-21T21:51:22.220839Z",
     "iopub.status.busy": "2026-02-21T21:51:22.220374Z",
     "iopub.status.idle": "2026-02-21T21:51:22.225735Z",
     "shell.execute_reply": "2026-02-21T21:51:22.225004Z",
     "shell.execute_reply.started": "2026-02-21T21:51:22.220800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for model in synth_data.keys():\n",
    "    print(f\"{model} - {synth_data[model][\"x\"].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14458775,
     "datasetId": 8723688,
     "isSourceIdPinned": false,
     "sourceId": 13712791,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
