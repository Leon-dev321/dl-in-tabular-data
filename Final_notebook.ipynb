{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3eafef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu124\n",
      "Uninstalling torch-2.6.0+cu124:\n",
      "  Successfully uninstalled torch-2.6.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Skipping torchvision as it is not installed.\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp313-cp313-win_amd64.whl (2532.3 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.6.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (1.2.8)\n",
      "Requirement already satisfied: pytorch-tabnet in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: sdv in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (1.33.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (2.1.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (1.6.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.28 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (1.42.44)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.31 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (1.42.44)\n",
      "Requirement already satisfied: cloudpickle>=2.1.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (3.0.0)\n",
      "Requirement already satisfied: copulas>=0.12.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (0.14.1)\n",
      "Requirement already satisfied: ctgan>=0.11.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (0.12.0)\n",
      "Requirement already satisfied: deepecho>=0.7.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (0.8.0)\n",
      "Requirement already satisfied: rdt>=1.18.2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (1.20.0)\n",
      "Requirement already satisfied: sdmetrics>=0.21.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (0.27.0)\n",
      "Requirement already satisfied: platformdirs>=4.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (4.3.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sdv) (6.0.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.28->sdv) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.28->sdv) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from botocore<2.0.0,>=1.31->sdv) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from botocore<2.0.0,>=1.31->sdv) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from plotly->catboost) (9.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from plotly->catboost) (24.2)\n",
      "Requirement already satisfied: Faker!=37.11.0,>=17 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from rdt>=1.18.2->sdv) (40.4.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\utilizador\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~mpy (c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "!{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "!{sys.executable} -m pip install catboost pytorch-tabnet sdv xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7148143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import save_data as save_data\n",
    "\n",
    "#PREPROCESSING\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, StratifiedKFold\n",
    "\n",
    "#GENERATIVE MODELS\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer, CopulaGANSynthesizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#PREDICATIVE MODELS\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#METRICS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    " \n",
    "#DATASETS\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfc3c8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f305150",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 43\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "model_gen = \"\"\n",
    "#ratios = [0, 0.33, 0.67, 1.0]\n",
    "n_samples_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd3b26",
   "metadata": {},
   "source": [
    "# Helper Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d45a4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_to_dataframe(target, dataset=None, path=None):\n",
    "    if path is not None:\n",
    "        if '.csv' in path[-4:]:\n",
    "            df = pd.read_csv(path)\n",
    "            df.rename(columns={target: 'Target'}, inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            return concat_sets(path)\n",
    "    \n",
    "    if dataset is None:\n",
    "        raise ValueError(\"Missing one of the following parameters: 'dataset' ou 'path'.\")\n",
    "\n",
    "    try:\n",
    "        if hasattr(dataset, 'data') and hasattr(dataset, 'feature_names'): \n",
    "            df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "        elif isinstance(dataset, pd.DataFrame):\n",
    "            df = dataset.copy() \n",
    "        elif isinstance(dataset, np.ndarray):\n",
    "            df = pd.DataFrame(dataset)\n",
    "        elif isinstance(dataset, tuple):\n",
    "            df = pd.concat([dataset[0], dataset[1]], axis=1)\n",
    "        else:\n",
    "            raise TypeError(f\"There's no conversion for '{type(dataset).__name__}'.\")\n",
    "\n",
    "\n",
    "        if target in df.columns:\n",
    "            df.rename(columns={target: 'Target'}, inplace=True)\n",
    "        else:\n",
    "            print(f\"The given target '{target}' it's not a column of the dataset.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff7d54d",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f86c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Graph\n",
    "def show_corr_graph(data, cat_cols, ord_cols):\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    encoder_nominal = OneHotEncoder(sparse_output = False, handle_unknown = \"ignore\")\n",
    "    encoder_ordinal = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "\n",
    "    encoded_ordinal_cols = encoder_ordinal.fit_transform(data_copy[ord_cols])\n",
    "\n",
    "    data_copy[ord_cols] = encoded_ordinal_cols\n",
    "\n",
    "    encoded_nominal_cols = encoder_nominal.fit_transform(data_copy[cat_cols])\n",
    "\n",
    "    encoded_nominal_cols  = pd.DataFrame(\n",
    "        encoded_nominal_cols, \n",
    "        columns=encoder_nominal.get_feature_names_out(cat_cols),\n",
    "        index=data_copy.index\n",
    "    )\n",
    "    \n",
    "    # Replace categorical columns with encoded ones\n",
    "    data_copy = pd.concat([data_copy.drop(cat_cols, axis=1), encoded_nominal_cols], axis=1)\n",
    "    \n",
    "    corr_target = data_copy.corr()['Target'].drop('Target')\n",
    "    \n",
    "    plt.figure(figsize=(8, 18))\n",
    "    corr_target.sort_values().plot(kind='barh', fontsize=8)\n",
    "    plt.title('Feature correlation with target variable')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f70f76",
   "metadata": {},
   "source": [
    "## Data Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "370dcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner():\n",
    "    \n",
    "    def show_and_impute_missing_data(self, df, mode='mean'):\n",
    "        plt.figure(figsize=(18,4),dpi=300) \n",
    "        null_count = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "        null_count = null_count[null_count > 0]\n",
    "\n",
    "        if (len(null_count) > 0):\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            sns.barplot(y=null_count.values, x=null_count.index, palette='viridis')\n",
    "            plt.title('Number of null values per features')\n",
    "            plt.xlabel('Total of Nulls')\n",
    "            plt.ylabel('Features')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.show()\n",
    "\n",
    "            for cloumn in null_count.index:\n",
    "                if df[cloumn].dtype == 'object' or str(df[cloumn].dtype).startswith('category'):\n",
    "                    df[cloumn] = df[cloumn].fillna(df[cloumn].mode()[0])\n",
    "                elif mode=='median':\n",
    "                    df[cloumn] = df[cloumn].fillna(df[cloumn].median())\n",
    "                elif mode=='mean':\n",
    "                    df[cloumn] = df[cloumn].fillna(df[cloumn].mean())\n",
    "\n",
    "        else:\n",
    "            print(\"There is no value missing.\")\n",
    "\n",
    "    def remove_duplicated_data(self, df):\n",
    "        num_dupli_lines = df.duplicated().sum()\n",
    "        print(f\"There are {num_dupli_lines} duplicated samples that will be removed\")\n",
    "\n",
    "        if (num_dupli_lines > 0 ):\n",
    "            df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc87fe9",
   "metadata": {},
   "source": [
    "## Data Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3339e",
   "metadata": {},
   "source": [
    "### Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fe3ef",
   "metadata": {},
   "source": [
    "#### DDPM\n",
    "Denoising Diffusion Probabilistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d845386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "  \n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "  \n",
    "        self.input_embed = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.block2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        self.block3 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU())\n",
    "        \n",
    "\n",
    "        self.final = nn.Linear(hidden_dim, input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        t_emb = self.time_embed(t.float().view(-1, 1))\n",
    "        x_emb = self.input_embed(x)\n",
    "        h = self.activation(x_emb + t_emb)\n",
    "        \n",
    "\n",
    "        h = h + self.block1(h) \n",
    "        h = h + self.block2(h)\n",
    "        h = h + self.block3(h)\n",
    "        return self.final(h)\n",
    "\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, input_dim, n_steps=100, device='cpu'):\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        self.model = DiffusionNetwork(input_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.beta = torch.linspace(1e-4, 0.02, n_steps).to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def train_one_batch(self, x0):\n",
    "        self.model.train()\n",
    " \n",
    "        t = torch.randint(0, self.n_steps, (x0.shape[0],), device=self.device).long()\n",
    "\n",
    "        epsilon = torch.randn_like(x0)\n",
    "      \n",
    "        sqrt_alpha = torch.sqrt(self.alpha_hat[t]).view(-1, 1)\n",
    "        sqrt_one_minus = torch.sqrt(1 - self.alpha_hat[t]).view(-1, 1)\n",
    "        x_t = sqrt_alpha * x0 + sqrt_one_minus * epsilon\n",
    "  \n",
    "        pred_epsilon = self.model(x_t, t.float() / self.n_steps)\n",
    "\n",
    "        loss = self.criterion(pred_epsilon, epsilon)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples):\n",
    "        self.model.eval()\n",
    "        x = torch.randn((n_samples, self.model.final.out_features)).to(self.device)\n",
    "        for i in reversed(range(self.n_steps)):\n",
    "            t = torch.tensor([i] * n_samples, device=self.device)\n",
    "            pred_eps = self.model(x, t.float() / self.n_steps)\n",
    "            alpha_t, alpha_hat_t, beta_t = self.alpha[i], self.alpha_hat[i], self.beta[i]\n",
    "            \n",
    "            noise = torch.randn_like(x) if i > 0 else torch.zeros_like(x)\n",
    "            term1 = 1 / torch.sqrt(alpha_t)\n",
    "            term2 = (1 - alpha_t) / torch.sqrt(1 - alpha_hat_t)\n",
    "            x = term1 * (x - term2 * pred_eps) + torch.sqrt(beta_t) * noise\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_synthetic_data_ddpm(X_in, y_in, n_samples_ratio=0.5, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray() \n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "    \n",
    "\n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "    \n",
    "\n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "    \n",
    "\n",
    "    internal_scaler = StandardScaler()\n",
    "    data_scaled = internal_scaler.fit_transform(data_combined)\n",
    "    \n",
    "\n",
    "    dataset = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    ddpm = DiffusionModel(input_dim=dataset.shape[1], n_steps=50, device=device)\n",
    "    \n",
    "    batch_size = min(256, len(dataset))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            ddpm.train_one_batch(batch)\n",
    "            \n",
    "\n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "    generated_scaled = ddpm.sample(n_samples).cpu().numpy()\n",
    "    \n",
    "\n",
    "    generated_data = internal_scaler.inverse_transform(generated_scaled)\n",
    "    \n",
    "\n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "    \n",
    "    return X_syn, y_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5593391",
   "metadata": {},
   "source": [
    "#### GAN\n",
    "Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "27dee4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def get_synthetic_data_gan(X_in, y_in, n_samples_ratio=0.5, epochs=100, latent_dim=64):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray()\n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "    \n",
    "\n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "    \n",
    "\n",
    "    internal_scaler = StandardScaler()\n",
    "    data_scaled = internal_scaler.fit_transform(data_combined)\n",
    "    \n",
    "\n",
    "    real_data = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    data_dim = real_data.shape[1]\n",
    "    \n",
    "\n",
    "    generator = Generator(latent_dim, data_dim).to(device)\n",
    "    discriminator = Discriminator(data_dim).to(device)\n",
    "    \n",
    "    # Optimizers\n",
    "    lr = 0.0002\n",
    "    opt_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss() \n",
    "    \n",
    "    batch_size = min(128, len(real_data))\n",
    "    n_batches = int(np.ceil(len(real_data) / batch_size))\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(real_data))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            idx = indices[i*batch_size : (i+1)*batch_size]\n",
    "            real_batch = real_data[idx]\n",
    "            curr_batch_size = real_batch.size(0)\n",
    "\n",
    "\n",
    "            real_labels = torch.ones(curr_batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(curr_batch_size, 1).to(device)\n",
    "\n",
    "\n",
    "            opt_d.zero_grad()\n",
    "            \n",
    "\n",
    "            outputs = discriminator(real_batch)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "\n",
    "            z = torch.randn(curr_batch_size, latent_dim).to(device)\n",
    "            fake_batch = generator(z)\n",
    "            outputs = discriminator(fake_batch.detach()) \n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "            \n",
    "\n",
    "            opt_g.zero_grad()\n",
    "            \n",
    "            outputs = discriminator(fake_batch)\n",
    "            g_loss = criterion(outputs, real_labels) \n",
    "            \n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "\n",
    "    generator.eval()\n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim).to(device)\n",
    "        generated_scaled = generator(z).cpu().numpy()\n",
    "        \n",
    "    generated_data = internal_scaler.inverse_transform(generated_scaled)\n",
    "    \n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "    \n",
    "    return X_syn, y_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95b158",
   "metadata": {},
   "source": [
    "#### VAE\n",
    "Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43434f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.act(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.act(self.fc2(z))\n",
    "        return self.fc3(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "\n",
    "def vae_loss(x, x_hat, mu, logvar):\n",
    "    recon_loss = nn.MSELoss()(x_hat, x)\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl\n",
    "\n",
    "\n",
    "def get_synthetic_data_vae(\n",
    "    X_in, y_in,\n",
    "    n_samples_ratio=0.5,\n",
    "    epochs=80,\n",
    "    latent_dim=16,\n",
    "    hidden_dim=128,\n",
    "):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if hasattr(X_in, \"toarray\"): X_in = X_in.toarray()\n",
    "    if hasattr(X_in, \"values\"): X_in = X_in.values\n",
    "    if hasattr(y_in, \"values\"): y_in = y_in.values\n",
    "\n",
    "    y_in = y_in.reshape(-1, 1)\n",
    "\n",
    "    data_combined = np.hstack([X_in, y_in])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_scaled = scaler.fit_transform(data_combined)\n",
    "\n",
    "    data_t = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
    "    input_dim = data_t.shape[1]\n",
    "\n",
    "    model = VAE(input_dim, latent_dim, hidden_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    batch_size = min(256, len(data_t))\n",
    "    loader = torch.utils.data.DataLoader(data_t, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch in loader:\n",
    "            x_hat, mu, logvar = model(batch)\n",
    "            loss = vae_loss(batch, x_hat, mu, logvar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Generate synthetic samples\n",
    "    model.eval()\n",
    "    n_samples = int(len(X_in) * n_samples_ratio)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim).to(device)\n",
    "        generated_scaled = model.decode(z).cpu().numpy()\n",
    "\n",
    "    # Inverse scale\n",
    "    generated_data = scaler.inverse_transform(generated_scaled)\n",
    "\n",
    "    # Split back\n",
    "    X_syn = generated_data[:, :-1]\n",
    "    y_syn = generated_data[:, -1]\n",
    "\n",
    "    return X_syn, y_syn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce926b88",
   "metadata": {},
   "source": [
    "#### SDV (Synthetic Data Vault - library)\n",
    "- CTGAN\n",
    "- TVAE\n",
    "- CopulaGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "64a2cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synthetic_data_sdv(X_in, y_in, model_name=\"ctgan\", n_samples_ratio=0.5):\n",
    "    if hasattr(X_in, \"toarray\"):\n",
    "        X_in = X_in.toarray()\n",
    "\n",
    "\n",
    "    X_arr = X_in.values if hasattr(X_in, \"values\") else X_in\n",
    "    y_arr = y_in.values if hasattr(y_in, \"values\") else y_in\n",
    "    y_arr = y_arr.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    n_features = X_arr.shape[1]\n",
    "    col_names = [f\"col_{i}\" for i in range(n_features)] + [\"target\"]\n",
    "    \n",
    "    data_combined = np.hstack([X_arr, y_arr])\n",
    "    df_temp = pd.DataFrame(data_combined, columns=col_names)\n",
    "\n",
    "\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=df_temp)\n",
    "\n",
    "    is_using_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    if model_name.lower() == \"tvae\":\n",
    "        model = TVAESynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu\n",
    "        )\n",
    "    elif model_name.lower() == \"copulagan\":\n",
    "        model = CopulaGANSynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu\n",
    "        )\n",
    "    else:\n",
    "        model = CTGANSynthesizer(\n",
    "            metadata,\n",
    "            enable_gpu=is_using_gpu,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(df_temp)\n",
    "    \n",
    "    n_samples = int(len(X_arr) * n_samples_ratio)\n",
    "    synthetic_df = model.sample(num_rows=n_samples)\n",
    "\n",
    "\n",
    "    data_syn = synthetic_df.values\n",
    "    \n",
    "    X_syn = data_syn[:, :-1]\n",
    "    y_syn = data_syn[:, -1]\n",
    "\n",
    "    return X_syn, y_syn\n",
    "\n",
    "\n",
    "def get_synthetic_data_ctgan(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"ctgan\", n_samples_ratio=n_samples_ratio)\n",
    "\n",
    "def get_synthetic_data_tvae(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"tvae\", n_samples_ratio=n_samples_ratio)\n",
    "\n",
    "def get_synthetic_data_copulagan(X_in, y_in, n_samples_ratio=0.5):\n",
    "    return get_synthetic_data_sdv(X_in, y_in, model_name=\"copulagan\", n_samples_ratio=n_samples_ratio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7577cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_params = {\n",
    "    \"DDPM\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\": 100\n",
    "    },\n",
    "    \"GAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\": 100, \n",
    "        \"latent_dim\": 64\n",
    "    },\n",
    "    \"VAE\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "        \"epochs\":80,\n",
    "        \"latent_dim\":16,\n",
    "        \"hidden_dim\":128,\n",
    "    },\n",
    "    \"CTGAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    },\n",
    "    \"TVAE\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    },\n",
    "    \"COPULAGAN\" : {\n",
    "        \"n_samples_ratio\": n_samples_ratio,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb207fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(model_name, X_train, y_train, n_samples_ratio = 0.5, params={}):\n",
    "    if model_name.upper() in gen_model_params.keys():\n",
    "        params = gen_model_params[model_name.upper()]\n",
    "        params[\"n_samples_ratio\"] = n_samples_ratio\n",
    "    else:\n",
    "        raise ValueError(f\"The generative model {model_name} is unknown.\")\n",
    "    \n",
    "\n",
    "    if model_name.upper() == \"DDPM\":\n",
    "        X_syn, y_syn = get_synthetic_data_ddpm(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn\n",
    "    elif model_name.upper() == \"GAN\":\n",
    "        X_syn, y_syn = get_synthetic_data_gan(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn\n",
    "    elif model_name.upper() == \"VAE\":\n",
    "        X_syn, y_syn = get_synthetic_data_vae(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn\n",
    "    elif model_name.upper() == \"COPULAGAN\":\n",
    "        X_syn, y_syn = get_synthetic_data_copulagan(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn\n",
    "    elif model_name.upper() == \"TVAE\":\n",
    "        X_syn, y_syn = get_synthetic_data_tvae(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn\n",
    "    else: \n",
    "        X_syn, y_syn = get_synthetic_data_ctgan(X_train, y_train, **params)\n",
    "        X_tr = np.vstack([X_train, X_syn])\n",
    "        y_tr = np.concatenate([y_train.ravel(), y_syn])\n",
    "        return X_tr, y_tr, X_syn, y_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839d8f2",
   "metadata": {},
   "source": [
    "## Predicative Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "13fd5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    if (dataset_name == \"California Housing\"):\n",
    "        match model_name:\n",
    "            case \"catboost\":\n",
    "                return CatBoostRegressor(\n",
    "                    loss_function=\"RMSE\",\n",
    "                    random_seed=RANDOM_STATE,\n",
    "                    verbose=0,\n",
    "                    bootstrap_type='Bayesian',\n",
    "                    depth=12,\n",
    "                    learning_rate=0.07992092259974018,\n",
    "                    l2_leaf_reg=6.046365280425728,\n",
    "                    boosting_type='Ordered',\n",
    "                    iterations=192,\n",
    "                    bagging_temperature=0.39306091938584564,\n",
    "                    task_type=\"GPU\" if use_cuda else \"CPU\",\n",
    "                    colsample_bylevel=1.0, \n",
    "                    devices='0' if use_cuda else None\n",
    "                )\n",
    "\n",
    "            case \"lightgbm\":\n",
    "                return LGBMRegressor(\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    n_estimators=500,\n",
    "                    learning_rate=0.049999324063844684,\n",
    "                    num_leaves=256,\n",
    "                    min_child_samples=5,\n",
    "                    verbose=-1,\n",
    "                    device=\"gpu\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "\n",
    "            case \"xgboost\":\n",
    "                return XGBRegressor(\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    learning_rate=0.14994404641843717,\n",
    "                    n_estimators=1509,\n",
    "                    max_depth=10,\n",
    "                    min_child_weight=8.190356462232765,\n",
    "                    gamma=0.08814992927621271,\n",
    "                    reg_lambda=3.426783661680087,\n",
    "                    reg_alpha=1.2318813880224584,\n",
    "                    subsample=0.9026614605709279,\n",
    "                    colsample_bytree=0.7927842877720923,\n",
    "                    tree_method=\"hist\",\n",
    "                    device=\"gpu\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "            \n",
    "            case \"tabnet\":\n",
    "                return TabNetRegressor(\n",
    "                    seed=RANDOM_STATE,\n",
    "                    n_d=38,\n",
    "                    n_a=40,\n",
    "                    n_steps=3,\n",
    "                    gamma=1.3358830131325452,\n",
    "                    lambda_sparse=3.068328401027138e-06,\n",
    "                    optimizer_params = dict(lr = 0.01926519792457367, weight_decay = 3.649170038293853e-06),\n",
    "                    mask_type='entmax',\n",
    "                    device_name=\"cuda\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"Model name unknown.\")\n",
    "    else:\n",
    "        match model_name:\n",
    "            case \"catboost\":\n",
    "                return CatBoostRegressor(\n",
    "                    loss_function=\"RMSE\",\n",
    "                    random_seed=RANDOM_STATE,\n",
    "                    verbose=0,\n",
    "                    learning_rate= 0.07806930282501494,\n",
    "                    iterations=789,\n",
    "                    depth=7,\n",
    "                    l2_leaf_reg=9.978159220509735,\n",
    "                    colsample_bylevel=0.7157670004918548,\n",
    "                    boosting_type=\"Ordered\",\n",
    "                    bootstrap_type=\"Bayesian\",\n",
    "                    bagging_temperature=2.818432439177726,\n",
    "                    task_type=\"GPU\" if use_cuda else \"CPU\", \n",
    "                    devices='0' if use_cuda else None\n",
    "                )\n",
    "\n",
    "            case \"lightgbm\":\n",
    "                return LGBMRegressor(\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    objective=\"regression\",\n",
    "                    metric=\"rmse\",\n",
    "                    boosting_type=\"gbdt\",\n",
    "                    n_estimators=1481,\n",
    "                    learning_rate=0.06998502190023687,\n",
    "                    num_leaves=64,\n",
    "                    min_child_samples=20,\n",
    "                    subsample=0.6980292009751712,\n",
    "                    colsample_bytree=0.8867225414334419,\n",
    "                    lambda_l1=0.3600170288892219,\n",
    "                    lambda_l2=1.0002607247863917,\n",
    "                    verbose=-1,\n",
    "                    device=\"gpu\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "            \n",
    "\n",
    "            case \"xgboost\":\n",
    "                return XGBRegressor(\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    eval_metric=\"RMSE\",\n",
    "                    n_jobs=-1,\n",
    "                    learning_rate=0.07994375879534497,\n",
    "                    n_estimators=960,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=4.464573553847174,\n",
    "                    gamma=0.8256486282421783,\n",
    "                    reg_lambda=5.001745986861921,\n",
    "                    reg_alpha=4.698752741479729,\n",
    "                    subsample=0.6139678010364236,\n",
    "                    colsample_bytree=0.7940319357294081,\n",
    "                    tree_method=\"hist\",\n",
    "                    device=\"gpu\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "\n",
    "            case \"tabnet\":\n",
    "                return TabNetRegressor(\n",
    "                    seed=RANDOM_STATE,\n",
    "                    n_d=15,\n",
    "                    n_a=13,\n",
    "                    n_steps=5,\n",
    "                    gamma=1.0717509193090469,\n",
    "                    lambda_sparse=0.00011663738596629411,\n",
    "                    optimizer_params = dict(lr = 0.008271431417721855),\n",
    "                    mask_type=\"entmax\",\n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    device_name=\"cuda\" if use_cuda else \"cpu\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20e71d",
   "metadata": {},
   "source": [
    "## Stacking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ab747a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def train_and_test_model(model_name, X_tr, y_tr, X_test): \n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    if model_name != \"tabnet\":\n",
    "        model.fit(X_tr, y_tr)\n",
    "    else:\n",
    "         model.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    eval_metric=['rmse'],\n",
    "                    patience=50,\n",
    "                    max_epochs=200,\n",
    "                    batch_size=512 if dataset_name == \"California Housing\" else 128,\n",
    "                    virtual_batch_size=128 if dataset_name == \"California Housing\" else 64,\n",
    "                )\n",
    "        \n",
    "    model_preds = model.predict(X_test).reshape(-1,1)    \n",
    "\n",
    "    return model_preds\n",
    "\"\"\"\n",
    "\n",
    "def train_pred_model(model_name, X_tr, y_tr):\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    if model_name != \"tabnet\":\n",
    "        model.fit(X_tr, y_tr)\n",
    "    else:\n",
    "        model.fit(\n",
    "                X_tr, y_tr,\n",
    "                eval_metric=['rmse'],\n",
    "                patience=50,\n",
    "                max_epochs=200,\n",
    "                batch_size=1024 if dataset_name == \"California Housing\" else 128,\n",
    "                virtual_batch_size=256 if dataset_name == \"California Housing\" else 64,\n",
    "            )\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_pred_model(model, X_test):\n",
    "    return model.predict(X_test).reshape(-1,1)   \n",
    "            \n",
    "def get_oof_preds(kfold, X_train, y_train, model_pred_name, model_gen_name=\"ctgan\", n_samples_ratio=0, doAugment=False):\n",
    "    oof_models_list = []\n",
    "\n",
    "    #will store the oof preds on all validation sets\n",
    "    oof_preds = np.zeros(len(y_train))\n",
    "    \n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(kfold):\n",
    "        # will store previsions for the actual valiadtion set\n",
    "        preds_on_fold = np.zeros(len(val_idx))\n",
    "        X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "        \n",
    "        if model_gen != \"\":\n",
    "            #data augmentation with generative\n",
    "            synthetic_data_type = \"trees\" if model_pred_name != \"tabnet\" else \"tabnet\"\n",
    "            folder = os.path.join(\"synthetic_data\", dataset_name, synthetic_data_type, model_gen, f\"ratio_{n_samples_ratio}\")\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "    \n",
    "            file_path = os.path.join(folder, f\"fold_{fold_idx}.json\")\n",
    "\n",
    "            if not doAugment and os.path.exists(file_path):\n",
    "                print(\"Existe o Fold\")\n",
    "\n",
    "                X_syn, y_syn, tr_idx_test, val_idx_test = save_data.load_synthetic_fold(\n",
    "                                                        model_gen=model_gen,\n",
    "                                                        dataset=dataset_name,\n",
    "                                                        ratio=n_samples_ratio,\n",
    "                                                        fold_idx=fold_idx,\n",
    "                                                        isTrees=True\n",
    "                                                    )\n",
    "                \n",
    "                if np.array_equal(tr_idx, tr_idx_test):\n",
    "                    print(\"So iguais\")\n",
    "\n",
    "                X_tr_aug = np.vstack([X_tr, X_syn])\n",
    "                y_tr_aug = np.concatenate([y_tr.ravel(), y_syn])   \n",
    "                \n",
    "            else:\n",
    "                X_tr_aug, y_tr_aug, X_syn, y_syn = augment_data(\n",
    "                    X_train=X_tr, \n",
    "                    y_train=y_tr, \n",
    "                    model_name=model_gen_name, \n",
    "                    n_samples_ratio=n_samples_ratio\n",
    "                )\n",
    "\n",
    "                save_data.save_fold_to_json(\n",
    "                    X_syn = X_syn,\n",
    "                    y_syn = y_syn,\n",
    "                    tr_idx = tr_idx,\n",
    "                    val_idx = val_idx,\n",
    "                    model_gen = model_gen,\n",
    "                    ratio = n_samples_ratio,\n",
    "                    fold_idx = fold_idx,\n",
    "                    dataset=dataset_name,\n",
    "                    isTrees= True if model_pred_name != \"tabnet\" else False\n",
    "                )\n",
    "\n",
    "        \n",
    "        #preds_on_fold = train_and_test_model(model_pred_name, X_tr, y_tr.reshape(-1, 1) if model_pred_name is \"tabnet\" else y_tr, X_val)\n",
    "        if model_gen != \"\":\n",
    "            trained_model = train_pred_model(\n",
    "                                model_name = model_pred_name,\n",
    "                                X_tr = X_tr_aug,\n",
    "                                y_tr = y_tr_aug.reshape(-1, 1) if model_pred_name == \"tabnet\" else y_tr_aug\n",
    "                            )\n",
    "        else:\n",
    "            trained_model = train_pred_model(\n",
    "                                model_name = model_pred_name,\n",
    "                                X_tr = X_tr,\n",
    "                                y_tr = y_tr.reshape(-1, 1) if model_pred_name == \"tabnet\" else y_tr_aug\n",
    "                            )\n",
    "        \n",
    "        oof_models_list.append(trained_model)\n",
    "        preds_on_fold = test_pred_model(trained_model, X_val)\n",
    "\n",
    "        oof_preds[val_idx.reshape(-1,1)] = preds_on_fold\n",
    "    \n",
    "    save_data.save_oof_models(\n",
    "        dataset_name= dataset_name,\n",
    "        models_list = oof_models_list,\n",
    "        model_pred = model_pred_name,\n",
    "        model_gen = model_gen\n",
    "    )\n",
    "\n",
    "    return oof_preds.reshape(-1,1)\n",
    "\n",
    "def train_meta_model(X_tr, y_tr):\n",
    "    meta_model = Ridge()\n",
    "    meta_model.fit(X_tr, y_tr)\n",
    "\n",
    "    return meta_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2b7485a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTracker:\n",
    "    def __init__(self, filepath=\"./Metrics/metrics.csv\"):\n",
    "        self.filepath = filepath\n",
    "        self.columns = [\n",
    "            \"Dataset\",\"Generative Model\", \"Predicative Model\", \"Synthetic Data Ratio\",\n",
    "            \"MAE\", \"MSE\", \"RMSE\", \"R2\"\n",
    "        ]\n",
    "        self.metrics_df = self._load_file()\n",
    "\n",
    "    def _load_file(self):\n",
    "        if os.path.exists(self.filepath):\n",
    "            return pd.read_csv(self.filepath)\n",
    "        return pd.DataFrame(columns=self.columns)\n",
    "\n",
    "    def add_metrics(self, dataset, y_true, y_pred, pred_name, gen_name, ratio=0):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        \n",
    "        target_gen_name = gen_name if gen_name != \"\" else \"-\"\n",
    "        \n",
    "        \n",
    "        new_row = [dataset, target_gen_name, pred_name, ratio, mae, mse, rmse, r2]\n",
    "\n",
    "        \n",
    "        mask = (\n",
    "            (self.metrics_df[\"Dataset\"] == dataset) &\n",
    "            (self.metrics_df[\"Generative Model\"] == target_gen_name) & \n",
    "            (self.metrics_df[\"Predicative Model\"] == pred_name) &\n",
    "            (self.metrics_df[\"Synthetic Data Ratio\"] == ratio)\n",
    "        )\n",
    "\n",
    "        if mask.any():\n",
    "            self.metrics_df.loc[mask] = new_row\n",
    "            print(\"Updated an existing row\")\n",
    "        else:\n",
    "            print(\"New model metrics added\")\n",
    "            self.metrics_df.loc[len(self.metrics_df)] = new_row\n",
    "        \n",
    "        self.save()\n",
    "        \n",
    "    def save(self):\n",
    "        self.metrics_df.to_csv(self.filepath, index=False)\n",
    "        print(\"File Saved\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        return self.metrics_df.sort_values(by=\"R2\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cc1838d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Generative Model</th>\n",
       "      <th>Predicative Model</th>\n",
       "      <th>Synthetic Data Ratio</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275269</td>\n",
       "      <td>0.185445</td>\n",
       "      <td>0.430633</td>\n",
       "      <td>0.863098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275137</td>\n",
       "      <td>0.192930</td>\n",
       "      <td>0.439238</td>\n",
       "      <td>0.857572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.286503</td>\n",
       "      <td>0.198146</td>\n",
       "      <td>0.445136</td>\n",
       "      <td>0.853721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.283502</td>\n",
       "      <td>0.198185</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.853692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.282377</td>\n",
       "      <td>0.198832</td>\n",
       "      <td>0.445905</td>\n",
       "      <td>0.853215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.289142</td>\n",
       "      <td>0.199284</td>\n",
       "      <td>0.446412</td>\n",
       "      <td>0.852881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285197</td>\n",
       "      <td>0.199349</td>\n",
       "      <td>0.446485</td>\n",
       "      <td>0.852833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.290652</td>\n",
       "      <td>0.201168</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.851490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291499</td>\n",
       "      <td>0.201379</td>\n",
       "      <td>0.448753</td>\n",
       "      <td>0.851334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288232</td>\n",
       "      <td>0.203485</td>\n",
       "      <td>0.451093</td>\n",
       "      <td>0.849779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.286947</td>\n",
       "      <td>0.203705</td>\n",
       "      <td>0.451337</td>\n",
       "      <td>0.849617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.292119</td>\n",
       "      <td>0.204821</td>\n",
       "      <td>0.452571</td>\n",
       "      <td>0.848794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.293255</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.453436</td>\n",
       "      <td>0.848215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291875</td>\n",
       "      <td>0.209690</td>\n",
       "      <td>0.457919</td>\n",
       "      <td>0.845199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.301633</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.842190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.310913</td>\n",
       "      <td>0.223940</td>\n",
       "      <td>0.473223</td>\n",
       "      <td>0.834679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.314072</td>\n",
       "      <td>0.225565</td>\n",
       "      <td>0.474936</td>\n",
       "      <td>0.833480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.317982</td>\n",
       "      <td>0.230561</td>\n",
       "      <td>0.480167</td>\n",
       "      <td>0.829791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322342</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>0.488217</td>\n",
       "      <td>0.824037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>0.239741</td>\n",
       "      <td>0.489634</td>\n",
       "      <td>0.823014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.331311</td>\n",
       "      <td>0.249159</td>\n",
       "      <td>0.499158</td>\n",
       "      <td>0.816061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.398988</td>\n",
       "      <td>0.355158</td>\n",
       "      <td>0.595951</td>\n",
       "      <td>0.737809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.400457</td>\n",
       "      <td>0.357177</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.736318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.365938</td>\n",
       "      <td>0.604928</td>\n",
       "      <td>0.729851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.406634</td>\n",
       "      <td>0.374362</td>\n",
       "      <td>0.611852</td>\n",
       "      <td>0.723631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.418725</td>\n",
       "      <td>0.375949</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.722460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.416784</td>\n",
       "      <td>0.377534</td>\n",
       "      <td>0.614438</td>\n",
       "      <td>0.721290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.417490</td>\n",
       "      <td>0.381695</td>\n",
       "      <td>0.617815</td>\n",
       "      <td>0.718218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset Generative Model Predicative Model  \\\n",
       "2   California Housing                -           xgboost   \n",
       "3   California Housing                -          lightgbm   \n",
       "26  California Housing             ddpm           xgboost   \n",
       "19  California Housing              gan          lightgbm   \n",
       "27  California Housing             ddpm          lightgbm   \n",
       "18  California Housing              gan           xgboost   \n",
       "7   California Housing        copulagan          lightgbm   \n",
       "22  California Housing              vae           xgboost   \n",
       "6   California Housing        copulagan           xgboost   \n",
       "15  California Housing            ctgan          lightgbm   \n",
       "23  California Housing              vae          lightgbm   \n",
       "14  California Housing            ctgan           xgboost   \n",
       "10  California Housing             tvae           xgboost   \n",
       "11  California Housing             tvae          lightgbm   \n",
       "1   California Housing                -          catboost   \n",
       "25  California Housing             ddpm          catboost   \n",
       "5   California Housing        copulagan          catboost   \n",
       "13  California Housing            ctgan          catboost   \n",
       "9   California Housing             tvae          catboost   \n",
       "17  California Housing              gan          catboost   \n",
       "21  California Housing              vae          catboost   \n",
       "20  California Housing              vae            tabnet   \n",
       "24  California Housing             ddpm            tabnet   \n",
       "0   California Housing                -            tabnet   \n",
       "16  California Housing              gan            tabnet   \n",
       "12  California Housing            ctgan            tabnet   \n",
       "4   California Housing        copulagan            tabnet   \n",
       "8   California Housing             tvae            tabnet   \n",
       "\n",
       "    Synthetic Data Ratio       MAE       MSE      RMSE        R2  \n",
       "2                   0.00  0.275269  0.185445  0.430633  0.863098  \n",
       "3                   0.00  0.275137  0.192930  0.439238  0.857572  \n",
       "26                  0.25  0.286503  0.198146  0.445136  0.853721  \n",
       "19                  0.25  0.283502  0.198185  0.445180  0.853692  \n",
       "27                  0.25  0.282377  0.198832  0.445905  0.853215  \n",
       "18                  0.25  0.289142  0.199284  0.446412  0.852881  \n",
       "7                   0.25  0.285197  0.199349  0.446485  0.852833  \n",
       "22                  0.25  0.290652  0.201168  0.448518  0.851490  \n",
       "6                   0.25  0.291499  0.201379  0.448753  0.851334  \n",
       "15                  0.25  0.288232  0.203485  0.451093  0.849779  \n",
       "23                  0.25  0.286947  0.203705  0.451337  0.849617  \n",
       "14                  0.25  0.292119  0.204821  0.452571  0.848794  \n",
       "10                  0.25  0.293255  0.205604  0.453436  0.848215  \n",
       "11                  0.25  0.291875  0.209690  0.457919  0.845199  \n",
       "1                   0.00  0.301633  0.213766  0.462348  0.842190  \n",
       "25                  0.25  0.310913  0.223940  0.473223  0.834679  \n",
       "5                   0.25  0.314072  0.225565  0.474936  0.833480  \n",
       "13                  0.25  0.317982  0.230561  0.480167  0.829791  \n",
       "9                   0.25  0.322342  0.238356  0.488217  0.824037  \n",
       "17                  0.25  0.322549  0.239741  0.489634  0.823014  \n",
       "21                  0.25  0.331311  0.249159  0.499158  0.816061  \n",
       "20                  0.25  0.398988  0.355158  0.595951  0.737809  \n",
       "24                  0.25  0.400457  0.357177  0.597643  0.736318  \n",
       "0                   0.00  0.408754  0.365938  0.604928  0.729851  \n",
       "16                  0.25  0.406634  0.374362  0.611852  0.723631  \n",
       "12                  0.25  0.418725  0.375949  0.613147  0.722460  \n",
       "4                   0.25  0.416784  0.377534  0.614438  0.721290  \n",
       "8                   0.25  0.417490  0.381695  0.617815  0.718218  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_tracker = ModelTracker()\n",
    "display(models_tracker.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3106c",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f1b1f207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genero</th>\n",
       "      <th>estado_civil</th>\n",
       "      <th>zona_residencia</th>\n",
       "      <th>imc</th>\n",
       "      <th>fumador</th>\n",
       "      <th>class_etaria</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>masculino</td>\n",
       "      <td>casado</td>\n",
       "      <td>suburbana</td>\n",
       "      <td>35.8</td>\n",
       "      <td>sim</td>\n",
       "      <td>adulto_meia_idade</td>\n",
       "      <td>18213.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feminino</td>\n",
       "      <td>separado</td>\n",
       "      <td>urbana</td>\n",
       "      <td>39.3</td>\n",
       "      <td>nao</td>\n",
       "      <td>adulto_senior</td>\n",
       "      <td>4266.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>masculino</td>\n",
       "      <td>solteiro</td>\n",
       "      <td>rural</td>\n",
       "      <td>40.7</td>\n",
       "      <td>nao</td>\n",
       "      <td>jovem</td>\n",
       "      <td>854.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>masculino</td>\n",
       "      <td>casado</td>\n",
       "      <td>urbana</td>\n",
       "      <td>28.2</td>\n",
       "      <td>sim</td>\n",
       "      <td>adulto_meia_idade</td>\n",
       "      <td>10169.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feminino</td>\n",
       "      <td>solteiro</td>\n",
       "      <td>urbana</td>\n",
       "      <td>31.1</td>\n",
       "      <td>nao</td>\n",
       "      <td>adulto_senior</td>\n",
       "      <td>4151.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      genero estado_civil zona_residencia   imc fumador       class_etaria  \\\n",
       "0  masculino       casado       suburbana  35.8     sim  adulto_meia_idade   \n",
       "1   feminino     separado          urbana  39.3     nao      adulto_senior   \n",
       "2  masculino     solteiro           rural  40.7     nao              jovem   \n",
       "3  masculino       casado          urbana  28.2     sim  adulto_meia_idade   \n",
       "4   feminino     solteiro          urbana  31.1     nao      adulto_senior   \n",
       "\n",
       "    Target  \n",
       "0  18213.9  \n",
       "1   4266.4  \n",
       "2    854.6  \n",
       "3  10169.7  \n",
       "4   4151.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2215, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset_name = \"California Housing\"\n",
    "#data = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "#data = convert_dataset_to_dataframe(dataset=data, target = 'MedHouseVal')\n",
    "\n",
    "dataset_name = \"Custo Seguro\"\n",
    "path = './datasets/custo_seguro.csv'\n",
    "data = convert_dataset_to_dataframe(path=path, target = 'custo')\n",
    "\n",
    "display(data.head())\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e87d5f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imc</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2215.000000</td>\n",
       "      <td>2215.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.704199</td>\n",
       "      <td>6045.710971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.425918</td>\n",
       "      <td>8354.316703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.400000</td>\n",
       "      <td>487.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.150000</td>\n",
       "      <td>2042.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.400000</td>\n",
       "      <td>4050.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34.700000</td>\n",
       "      <td>7176.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.100000</td>\n",
       "      <td>180544.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               imc         Target\n",
       "count  2215.000000    2215.000000\n",
       "mean     30.704199    6045.710971\n",
       "std       6.425918    8354.316703\n",
       "min       3.400000     487.200000\n",
       "25%      26.150000    2042.200000\n",
       "50%      30.400000    4050.700000\n",
       "75%      34.700000    7176.300000\n",
       "max      83.100000  180544.400000"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef992ca8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9fedfdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no value missing.\n",
      "There are 1 duplicated samples that will be removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_cleaner = DataCleaner()\n",
    "\n",
    "data_cleaner.show_and_impute_missing_data(data)\n",
    "data_cleaner.remove_duplicated_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0f348479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['genero', 'estado_civil', 'zona_residencia', 'fumador', 'class_etaria']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['class_etaria']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x1800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.drop(columns=['Target'])\n",
    "y = data['Target']\n",
    "\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == 'object' or str(X[c].dtype).startswith('category')]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "ord_cols = []\n",
    "display(cat_cols)\n",
    "\n",
    "if \"class_etaria\" in cat_cols:\n",
    "    ord_cols.append(\"class_etaria\")\n",
    "    cat_cols.remove(\"class_etaria\")\n",
    "\n",
    "display(ord_cols)\n",
    "\n",
    "show_corr_graph(data, cat_cols, ord_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0735c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratification for regression\n",
    "n_bins = 10 \n",
    "y_binned = pd.qcut(y, q=n_bins, duplicates='drop')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=RANDOM_STATE, stratify=y_binned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb655f",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "70533bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_trees = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "ordinal_trees = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "scaler_standard = StandardScaler()\n",
    "\n",
    "preprocess_for_trees = ColumnTransformer(\n",
    "   transformers=[(\"onehot_trees\", onehot_trees, cat_cols),\n",
    "                 (\"ordinal_trees\", ordinal_trees, ord_cols)],\n",
    "   remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "onehot_tab = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "ordinal_tab = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "preprocess_for_tab = ColumnTransformer(\n",
    "   transformers=[(\"onehot_encoder\", onehot_tab, cat_cols),\n",
    "                 (\"ordinal_trees\", ordinal_tab, ord_cols),\n",
    "                 (\"scaler_standard\", scaler_standard, num_cols)\n",
    "                ],\n",
    "   remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4e441ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kfold = KFold(n_splits=n_splits, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "#Stratification for CV\n",
    "n_bins = 10 \n",
    "bins = pd.Series(pd.qcut(y_train, q=n_bins, duplicates='drop')).cat.codes\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "folds = list(skf.split(X_train, bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5cc09934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transformation for trees\n",
    "X_train_trees = preprocess_for_trees.fit_transform(X_train)\n",
    "X_test_trees = preprocess_for_trees.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "74c93fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data transformatiom for tabnet\n",
    "X_train_tab = preprocess_for_tab.fit_transform(X_train)\n",
    "X_test_tab = preprocess_for_tab.transform(X_test)\n",
    "\n",
    "y_train_tab = y_train.values.astype(np.float32).reshape(-1,1)\n",
    "y_test_tab = y_test.values.astype(np.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b012a23",
   "metadata": {},
   "source": [
    "# Data Mining with Stacked Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68428f80",
   "metadata": {},
   "source": [
    "## Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b55461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126575753.66667|  0:00:00s\n",
      "epoch 1  | loss: 126363359.66667|  0:00:01s\n",
      "epoch 2  | loss: 125898600.33333|  0:00:01s\n",
      "epoch 3  | loss: 124271366.33333|  0:00:02s\n",
      "epoch 4  | loss: 126131973.0|  0:00:02s\n",
      "epoch 5  | loss: 118447945.33333|  0:00:03s\n",
      "epoch 6  | loss: 124916667.0|  0:00:03s\n",
      "epoch 7  | loss: 123804251.33333|  0:00:04s\n",
      "epoch 8  | loss: 122801575.66667|  0:00:04s\n",
      "epoch 9  | loss: 122572055.66667|  0:00:05s\n",
      "epoch 10 | loss: 123247939.33333|  0:00:05s\n",
      "epoch 11 | loss: 123376005.33333|  0:00:06s\n",
      "epoch 12 | loss: 122324403.66667|  0:00:06s\n",
      "epoch 13 | loss: 120865240.66667|  0:00:07s\n",
      "epoch 14 | loss: 117284404.0|  0:00:07s\n",
      "epoch 15 | loss: 120875329.33333|  0:00:08s\n",
      "epoch 16 | loss: 119727294.0|  0:00:08s\n",
      "epoch 17 | loss: 116549430.66667|  0:00:09s\n",
      "epoch 18 | loss: 117106158.33333|  0:00:09s\n",
      "epoch 19 | loss: 106603074.66667|  0:00:10s\n",
      "epoch 20 | loss: 115821120.33333|  0:00:10s\n",
      "epoch 21 | loss: 113990252.0|  0:00:11s\n",
      "epoch 22 | loss: 111727674.66667|  0:00:11s\n",
      "epoch 23 | loss: 111831268.66667|  0:00:11s\n",
      "epoch 24 | loss: 110467346.0|  0:00:12s\n",
      "epoch 25 | loss: 108750903.66667|  0:00:12s\n",
      "epoch 26 | loss: 93592149.33333|  0:00:13s\n",
      "epoch 27 | loss: 106982598.0|  0:00:13s\n",
      "epoch 28 | loss: 105248822.33333|  0:00:14s\n",
      "epoch 29 | loss: 103025079.66667|  0:00:14s\n",
      "epoch 30 | loss: 103140408.66667|  0:00:16s\n",
      "epoch 31 | loss: 102279071.33333|  0:00:18s\n",
      "epoch 32 | loss: 100518377.33333|  0:00:20s\n",
      "epoch 33 | loss: 98952505.83333|  0:00:21s\n",
      "epoch 34 | loss: 97314689.66667|  0:00:22s\n",
      "epoch 35 | loss: 95597165.83333|  0:00:22s\n",
      "epoch 36 | loss: 93736679.0|  0:00:22s\n",
      "epoch 37 | loss: 94292266.16667|  0:00:23s\n",
      "epoch 38 | loss: 92559663.0|  0:00:23s\n",
      "epoch 39 | loss: 86607697.33333|  0:00:24s\n",
      "epoch 40 | loss: 89762401.83333|  0:00:24s\n",
      "epoch 41 | loss: 89363097.33333|  0:00:25s\n",
      "epoch 42 | loss: 87496946.83333|  0:00:25s\n",
      "epoch 43 | loss: 69867202.16667|  0:00:26s\n",
      "epoch 44 | loss: 85354208.0|  0:00:26s\n",
      "epoch 45 | loss: 80194879.66667|  0:00:27s\n",
      "epoch 46 | loss: 82723513.83333|  0:00:27s\n",
      "epoch 47 | loss: 81578189.16667|  0:00:28s\n",
      "epoch 48 | loss: 71815911.83333|  0:00:28s\n",
      "epoch 49 | loss: 80269091.33333|  0:00:29s\n",
      "epoch 50 | loss: 80100392.0|  0:00:29s\n",
      "epoch 51 | loss: 77702650.25|  0:00:30s\n",
      "epoch 52 | loss: 73359088.08333|  0:00:30s\n",
      "epoch 53 | loss: 75835713.33333|  0:00:30s\n",
      "epoch 54 | loss: 75336093.08333|  0:00:31s\n",
      "epoch 55 | loss: 72986878.0|  0:00:31s\n",
      "epoch 56 | loss: 72679424.33333|  0:00:32s\n",
      "epoch 57 | loss: 72212334.83333|  0:00:32s\n",
      "epoch 58 | loss: 63328362.0|  0:00:33s\n",
      "epoch 59 | loss: 70697987.33333|  0:00:33s\n",
      "epoch 60 | loss: 65638592.125|  0:00:33s\n",
      "epoch 61 | loss: 68675547.91667|  0:00:34s\n",
      "epoch 62 | loss: 49951263.16667|  0:00:34s\n",
      "epoch 63 | loss: 66715616.25|  0:00:35s\n",
      "epoch 64 | loss: 65831690.20833|  0:00:35s\n",
      "epoch 65 | loss: 62928029.5|  0:00:36s\n",
      "epoch 66 | loss: 65251333.20833|  0:00:36s\n",
      "epoch 67 | loss: 62573050.91667|  0:00:37s\n",
      "epoch 68 | loss: 61622622.16667|  0:00:37s\n",
      "epoch 69 | loss: 54248478.375|  0:00:38s\n",
      "epoch 70 | loss: 61811434.91667|  0:00:38s\n",
      "epoch 71 | loss: 50619416.83333|  0:00:38s\n",
      "epoch 72 | loss: 59990996.04167|  0:00:39s\n",
      "epoch 73 | loss: 60017972.29167|  0:00:39s\n",
      "epoch 74 | loss: 57709179.83333|  0:00:40s\n",
      "epoch 75 | loss: 57530147.54167|  0:00:40s\n",
      "epoch 76 | loss: 57077328.9375|  0:00:41s\n",
      "epoch 77 | loss: 57939644.04167|  0:00:41s\n",
      "epoch 78 | loss: 58748228.08333|  0:00:42s\n",
      "epoch 79 | loss: 57585306.875|  0:00:42s\n",
      "epoch 80 | loss: 40170615.79167|  0:00:43s\n",
      "epoch 81 | loss: 56721120.70833|  0:00:43s\n",
      "epoch 82 | loss: 53470488.45833|  0:00:44s\n",
      "epoch 83 | loss: 57245721.45833|  0:00:44s\n",
      "epoch 84 | loss: 54203997.0|  0:00:44s\n",
      "epoch 85 | loss: 54196772.4375|  0:00:45s\n",
      "epoch 86 | loss: 55862056.89583|  0:00:45s\n",
      "epoch 87 | loss: 53803293.0625|  0:00:46s\n",
      "epoch 88 | loss: 49939841.25|  0:00:46s\n",
      "epoch 89 | loss: 53790092.20833|  0:00:47s\n",
      "epoch 90 | loss: 51712413.97917|  0:00:47s\n",
      "epoch 91 | loss: 51287876.64583|  0:00:48s\n",
      "epoch 92 | loss: 48825064.04167|  0:00:48s\n",
      "epoch 93 | loss: 53833181.97917|  0:00:48s\n",
      "epoch 94 | loss: 53616656.89583|  0:00:49s\n",
      "epoch 95 | loss: 50470132.92708|  0:00:49s\n",
      "epoch 96 | loss: 45448068.4375|  0:00:50s\n",
      "epoch 97 | loss: 52137575.625|  0:00:50s\n",
      "epoch 98 | loss: 48958230.66667|  0:00:51s\n",
      "epoch 99 | loss: 48583117.4375|  0:00:51s\n",
      "epoch 100| loss: 49231345.97917|  0:00:52s\n",
      "epoch 101| loss: 50252179.8125|  0:00:52s\n",
      "epoch 102| loss: 47664269.52083|  0:00:52s\n",
      "epoch 103| loss: 35465105.79167|  0:00:53s\n",
      "epoch 104| loss: 45693961.29167|  0:00:53s\n",
      "epoch 105| loss: 49512135.52083|  0:00:54s\n",
      "epoch 106| loss: 39621276.45833|  0:00:54s\n",
      "epoch 107| loss: 49614296.29167|  0:00:55s\n",
      "epoch 108| loss: 46368823.9375|  0:00:55s\n",
      "epoch 109| loss: 44825479.95833|  0:00:56s\n",
      "epoch 110| loss: 46148986.875|  0:00:56s\n",
      "epoch 111| loss: 34887565.6875|  0:00:56s\n",
      "epoch 112| loss: 52451832.9375|  0:00:57s\n",
      "epoch 113| loss: 48221332.72917|  0:00:58s\n",
      "epoch 114| loss: 50439340.69792|  0:00:58s\n",
      "epoch 115| loss: 47605087.5|  0:00:59s\n",
      "epoch 116| loss: 49072447.4375|  0:00:59s\n",
      "epoch 117| loss: 45397596.39583|  0:00:59s\n",
      "epoch 118| loss: 47072700.70833|  0:01:01s\n",
      "epoch 119| loss: 46521684.8125|  0:01:03s\n",
      "epoch 120| loss: 46048117.5|  0:01:04s\n",
      "epoch 121| loss: 41390244.41667|  0:01:06s\n",
      "epoch 122| loss: 46878211.58333|  0:01:08s\n",
      "epoch 123| loss: 41163321.375|  0:01:10s\n",
      "epoch 124| loss: 38906365.875|  0:01:12s\n",
      "epoch 125| loss: 49114676.02083|  0:01:14s\n",
      "epoch 126| loss: 42845177.35417|  0:01:16s\n",
      "epoch 127| loss: 27588635.79167|  0:01:17s\n",
      "epoch 128| loss: 43489758.97917|  0:01:19s\n",
      "epoch 129| loss: 51818443.04167|  0:01:21s\n",
      "epoch 130| loss: 38051037.45833|  0:01:23s\n",
      "epoch 131| loss: 42641124.5|  0:01:25s\n",
      "epoch 132| loss: 42598243.22917|  0:01:27s\n",
      "epoch 133| loss: 50124114.83333|  0:01:28s\n",
      "epoch 134| loss: 43071326.6875|  0:01:30s\n",
      "epoch 135| loss: 46609854.8125|  0:01:32s\n",
      "epoch 136| loss: 30582397.52083|  0:01:34s\n",
      "epoch 137| loss: 42907204.72917|  0:01:36s\n",
      "epoch 138| loss: 30958743.47917|  0:01:38s\n",
      "epoch 139| loss: 42363678.1875|  0:01:40s\n",
      "epoch 140| loss: 43839684.04167|  0:01:42s\n",
      "epoch 141| loss: 45835677.52083|  0:01:43s\n",
      "epoch 142| loss: 44684435.375|  0:01:45s\n",
      "epoch 143| loss: 41905475.02083|  0:01:47s\n",
      "epoch 144| loss: 41785495.4375|  0:01:49s\n",
      "epoch 145| loss: 49631525.9375|  0:01:51s\n",
      "epoch 146| loss: 43685322.97917|  0:01:53s\n",
      "epoch 147| loss: 44121465.25|  0:01:54s\n",
      "epoch 148| loss: 45057655.58333|  0:01:56s\n",
      "epoch 149| loss: 45977546.63542|  0:01:58s\n",
      "epoch 150| loss: 45274004.9375|  0:02:00s\n",
      "epoch 151| loss: 37887665.625|  0:02:02s\n",
      "epoch 152| loss: 42742825.69792|  0:02:04s\n",
      "epoch 153| loss: 43552576.125|  0:02:06s\n",
      "epoch 154| loss: 43553827.91667|  0:02:07s\n",
      "epoch 155| loss: 37717043.1875|  0:02:09s\n",
      "epoch 156| loss: 40940282.10417|  0:02:11s\n",
      "epoch 157| loss: 41920484.61458|  0:02:13s\n",
      "epoch 158| loss: 45241754.22917|  0:02:15s\n",
      "epoch 159| loss: 46086709.25|  0:02:17s\n",
      "epoch 160| loss: 38062814.20833|  0:02:18s\n",
      "epoch 161| loss: 44702190.0625|  0:02:20s\n",
      "epoch 162| loss: 44490371.70833|  0:02:22s\n",
      "epoch 163| loss: 42868692.89583|  0:02:23s\n",
      "epoch 164| loss: 45513462.71875|  0:02:25s\n",
      "epoch 165| loss: 43191465.0|  0:02:27s\n",
      "epoch 166| loss: 38891078.70833|  0:02:29s\n",
      "epoch 167| loss: 38215903.16667|  0:02:30s\n",
      "epoch 168| loss: 37035434.27083|  0:02:32s\n",
      "epoch 169| loss: 38615357.625|  0:02:34s\n",
      "epoch 170| loss: 39186381.41667|  0:02:35s\n",
      "epoch 171| loss: 39738565.70833|  0:02:37s\n",
      "epoch 172| loss: 42203714.08333|  0:02:39s\n",
      "epoch 173| loss: 37649446.5|  0:02:41s\n",
      "epoch 174| loss: 29021951.67708|  0:02:42s\n",
      "epoch 175| loss: 36959312.45833|  0:02:44s\n",
      "epoch 176| loss: 34752608.02083|  0:02:46s\n",
      "epoch 177| loss: 35771437.35417|  0:02:48s\n",
      "epoch 178| loss: 29982949.1875|  0:02:50s\n",
      "epoch 179| loss: 35019549.27083|  0:02:52s\n",
      "epoch 180| loss: 46628019.5|  0:02:54s\n",
      "epoch 181| loss: 29846761.125|  0:02:56s\n",
      "epoch 182| loss: 37110035.35417|  0:02:58s\n",
      "epoch 183| loss: 41512884.20833|  0:03:00s\n",
      "epoch 184| loss: 36459710.41667|  0:03:02s\n",
      "epoch 185| loss: 40469552.83333|  0:03:03s\n",
      "epoch 186| loss: 26638203.0625|  0:03:05s\n",
      "epoch 187| loss: 42339851.47917|  0:03:07s\n",
      "epoch 188| loss: 32193469.1875|  0:03:09s\n",
      "epoch 189| loss: 43666560.625|  0:03:11s\n",
      "epoch 190| loss: 41865412.0|  0:03:12s\n",
      "epoch 191| loss: 34708977.625|  0:03:14s\n",
      "epoch 192| loss: 34844548.35417|  0:03:16s\n",
      "epoch 193| loss: 40034630.125|  0:03:17s\n",
      "epoch 194| loss: 41826680.64583|  0:03:19s\n",
      "epoch 195| loss: 39752529.625|  0:03:21s\n",
      "epoch 196| loss: 37436035.08333|  0:03:23s\n",
      "epoch 197| loss: 36457232.375|  0:03:24s\n",
      "epoch 198| loss: 34494631.41667|  0:03:26s\n",
      "epoch 199| loss: 36730186.1875|  0:03:28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 116305947.66667|  0:00:01s\n",
      "epoch 1  | loss: 114185568.33333|  0:00:03s\n",
      "epoch 2  | loss: 110183674.66667|  0:00:04s\n",
      "epoch 3  | loss: 115946024.66667|  0:00:06s\n",
      "epoch 4  | loss: 116089986.0|  0:00:07s\n",
      "epoch 5  | loss: 116398850.0|  0:00:09s\n",
      "epoch 6  | loss: 115023472.0|  0:00:11s\n",
      "epoch 7  | loss: 115193207.0|  0:00:12s\n",
      "epoch 8  | loss: 114951849.66667|  0:00:14s\n",
      "epoch 9  | loss: 114168545.0|  0:00:16s\n",
      "epoch 10 | loss: 113675349.33333|  0:00:18s\n",
      "epoch 11 | loss: 113146837.0|  0:00:20s\n",
      "epoch 12 | loss: 112831860.0|  0:00:22s\n",
      "epoch 13 | loss: 112191158.0|  0:00:24s\n",
      "epoch 14 | loss: 111848682.33333|  0:00:26s\n",
      "epoch 15 | loss: 110132550.33333|  0:00:28s\n",
      "epoch 16 | loss: 110625624.0|  0:00:30s\n",
      "epoch 17 | loss: 102278452.0|  0:00:31s\n",
      "epoch 18 | loss: 108463438.66667|  0:00:33s\n",
      "epoch 19 | loss: 105614287.66667|  0:00:35s\n",
      "epoch 20 | loss: 88738159.0|  0:00:37s\n",
      "epoch 21 | loss: 106291422.33333|  0:00:39s\n",
      "epoch 22 | loss: 86008450.0|  0:00:41s\n",
      "epoch 23 | loss: 103184129.66667|  0:00:43s\n",
      "epoch 24 | loss: 101908018.33333|  0:00:44s\n",
      "epoch 25 | loss: 100800029.0|  0:00:46s\n",
      "epoch 26 | loss: 93277425.33333|  0:00:48s\n",
      "epoch 27 | loss: 97753511.16667|  0:00:50s\n",
      "epoch 28 | loss: 96374357.0|  0:00:52s\n",
      "epoch 29 | loss: 95658864.66667|  0:00:54s\n",
      "epoch 30 | loss: 91025589.33333|  0:00:55s\n",
      "epoch 31 | loss: 92072433.16667|  0:00:57s\n",
      "epoch 32 | loss: 73422372.5|  0:00:59s\n",
      "epoch 33 | loss: 89886835.33333|  0:01:01s\n",
      "epoch 34 | loss: 89104753.66667|  0:01:02s\n",
      "epoch 35 | loss: 83573570.5|  0:01:04s\n",
      "epoch 36 | loss: 85978473.33333|  0:01:06s\n",
      "epoch 37 | loss: 66166754.83333|  0:01:07s\n",
      "epoch 38 | loss: 82913996.33333|  0:01:09s\n",
      "epoch 39 | loss: 77129004.83333|  0:01:11s\n",
      "epoch 40 | loss: 79845821.5|  0:01:12s\n",
      "epoch 41 | loss: 62896651.16667|  0:01:14s\n",
      "epoch 42 | loss: 77521877.5|  0:01:16s\n",
      "epoch 43 | loss: 73933022.0|  0:01:18s\n",
      "epoch 44 | loss: 75284746.0|  0:01:20s\n",
      "epoch 45 | loss: 69902622.5|  0:01:21s\n",
      "epoch 46 | loss: 73436565.83333|  0:01:23s\n",
      "epoch 47 | loss: 70970679.66667|  0:01:24s\n",
      "epoch 48 | loss: 70804686.0|  0:01:24s\n",
      "epoch 49 | loss: 69425698.58333|  0:01:25s\n",
      "epoch 50 | loss: 67935152.58333|  0:01:25s\n",
      "epoch 51 | loss: 68174313.08333|  0:01:26s\n",
      "epoch 52 | loss: 67194113.08333|  0:01:26s\n",
      "epoch 53 | loss: 65073715.16667|  0:01:26s\n",
      "epoch 54 | loss: 63839003.75|  0:01:27s\n",
      "epoch 55 | loss: 61364796.79167|  0:01:27s\n",
      "epoch 56 | loss: 63518358.16667|  0:01:28s\n",
      "epoch 57 | loss: 56696072.58333|  0:01:28s\n",
      "epoch 58 | loss: 61288743.33333|  0:01:29s\n",
      "epoch 59 | loss: 60468151.29167|  0:01:29s\n",
      "epoch 60 | loss: 58826152.91667|  0:01:30s\n",
      "epoch 61 | loss: 58064703.0|  0:01:30s\n",
      "epoch 62 | loss: 37892677.33333|  0:01:30s\n",
      "epoch 63 | loss: 57306013.16667|  0:01:31s\n",
      "epoch 64 | loss: 53409210.20833|  0:01:31s\n",
      "epoch 65 | loss: 56206233.54167|  0:01:32s\n",
      "epoch 66 | loss: 53644056.91667|  0:01:32s\n",
      "epoch 67 | loss: 55029154.20833|  0:01:33s\n",
      "epoch 68 | loss: 55133173.125|  0:01:33s\n",
      "epoch 69 | loss: 52021744.625|  0:01:33s\n",
      "epoch 70 | loss: 51610296.83333|  0:01:34s\n",
      "epoch 71 | loss: 46741954.83333|  0:01:35s\n",
      "epoch 72 | loss: 51768239.41667|  0:01:35s\n",
      "epoch 73 | loss: 51637780.58333|  0:01:35s\n",
      "epoch 74 | loss: 50550713.70833|  0:01:36s\n",
      "epoch 75 | loss: 50696847.79167|  0:01:36s\n",
      "epoch 76 | loss: 49958475.04167|  0:01:37s\n",
      "epoch 77 | loss: 49371798.58333|  0:01:37s\n",
      "epoch 78 | loss: 47153577.5|  0:01:37s\n",
      "epoch 79 | loss: 47571027.64583|  0:01:38s\n",
      "epoch 80 | loss: 33171773.77083|  0:01:38s\n",
      "epoch 81 | loss: 47844387.04167|  0:01:39s\n",
      "epoch 82 | loss: 44145908.85417|  0:01:39s\n",
      "epoch 83 | loss: 48060483.1875|  0:01:40s\n",
      "epoch 84 | loss: 47304408.47917|  0:01:40s\n",
      "epoch 85 | loss: 46679908.52083|  0:01:41s\n",
      "epoch 86 | loss: 47957385.97917|  0:01:41s\n",
      "epoch 87 | loss: 45287024.1875|  0:01:41s\n",
      "epoch 88 | loss: 46550465.375|  0:01:42s\n",
      "epoch 89 | loss: 46933987.8125|  0:01:42s\n",
      "epoch 90 | loss: 46147367.39583|  0:01:43s\n",
      "epoch 91 | loss: 45600358.79167|  0:01:43s\n",
      "epoch 92 | loss: 45546026.22917|  0:01:44s\n",
      "epoch 93 | loss: 46885499.6875|  0:01:44s\n",
      "epoch 94 | loss: 44645607.47917|  0:01:45s\n",
      "epoch 95 | loss: 43962271.27083|  0:01:45s\n",
      "epoch 96 | loss: 42283298.08333|  0:01:45s\n",
      "epoch 97 | loss: 41815629.8125|  0:01:46s\n",
      "epoch 98 | loss: 44143084.66667|  0:01:46s\n",
      "epoch 99 | loss: 43472887.04167|  0:01:47s\n",
      "epoch 100| loss: 40120180.95833|  0:01:47s\n",
      "epoch 101| loss: 44242792.625|  0:01:48s\n",
      "epoch 102| loss: 43456313.6875|  0:01:48s\n",
      "epoch 103| loss: 39406696.5|  0:01:49s\n",
      "epoch 104| loss: 45898388.84375|  0:01:49s\n",
      "epoch 105| loss: 42600671.47917|  0:01:50s\n",
      "epoch 106| loss: 41034348.45833|  0:01:50s\n",
      "epoch 107| loss: 45278236.6875|  0:01:50s\n",
      "epoch 108| loss: 41604320.33333|  0:01:51s\n",
      "epoch 109| loss: 42571041.77083|  0:01:51s\n",
      "epoch 110| loss: 25346110.08333|  0:01:52s\n",
      "epoch 111| loss: 25603511.39583|  0:01:52s\n",
      "epoch 112| loss: 42871216.85417|  0:01:53s\n",
      "epoch 113| loss: 44281055.0625|  0:01:54s\n",
      "epoch 114| loss: 44746742.47917|  0:01:56s\n",
      "epoch 115| loss: 41201127.85417|  0:01:58s\n",
      "epoch 116| loss: 40597183.22917|  0:01:59s\n",
      "epoch 117| loss: 35040350.45833|  0:02:01s\n",
      "epoch 118| loss: 38495726.5625|  0:02:03s\n",
      "epoch 119| loss: 45583117.77083|  0:02:04s\n",
      "epoch 120| loss: 42278178.375|  0:02:06s\n",
      "epoch 121| loss: 35799337.97917|  0:02:08s\n",
      "epoch 122| loss: 37073928.85417|  0:02:09s\n",
      "epoch 123| loss: 44259840.29167|  0:02:09s\n",
      "epoch 124| loss: 38309921.125|  0:02:10s\n",
      "epoch 125| loss: 45740055.625|  0:02:10s\n",
      "epoch 126| loss: 43322128.97917|  0:02:11s\n",
      "epoch 127| loss: 38784317.4375|  0:02:11s\n",
      "epoch 128| loss: 39603698.77083|  0:02:12s\n",
      "epoch 129| loss: 43670265.16667|  0:02:12s\n",
      "epoch 130| loss: 40914107.02083|  0:02:12s\n",
      "epoch 131| loss: 38828000.25|  0:02:13s\n",
      "epoch 132| loss: 39133548.58333|  0:02:13s\n",
      "epoch 133| loss: 36756644.22917|  0:02:14s\n",
      "epoch 134| loss: 33061652.625|  0:02:14s\n",
      "epoch 135| loss: 39796204.0|  0:02:14s\n",
      "epoch 136| loss: 38262832.10417|  0:02:15s\n",
      "epoch 137| loss: 35381538.16667|  0:02:15s\n",
      "epoch 138| loss: 36905280.3125|  0:02:16s\n",
      "epoch 139| loss: 38013171.33333|  0:02:16s\n",
      "epoch 140| loss: 35101029.75|  0:02:17s\n",
      "epoch 141| loss: 36874411.02083|  0:02:17s\n",
      "epoch 142| loss: 36840610.54167|  0:02:18s\n",
      "epoch 143| loss: 37675445.72917|  0:02:19s\n",
      "epoch 144| loss: 32450253.19792|  0:02:20s\n",
      "epoch 145| loss: 36093347.5625|  0:02:21s\n",
      "epoch 146| loss: 25351417.66667|  0:02:21s\n",
      "epoch 147| loss: 33860913.10417|  0:02:22s\n",
      "epoch 148| loss: 37365007.52083|  0:02:22s\n",
      "epoch 149| loss: 37921880.34375|  0:02:23s\n",
      "epoch 150| loss: 34496996.58333|  0:02:23s\n",
      "epoch 151| loss: 36778187.14583|  0:02:23s\n",
      "epoch 152| loss: 33418279.22917|  0:02:24s\n",
      "epoch 153| loss: 31944340.16667|  0:02:24s\n",
      "epoch 154| loss: 34314448.4375|  0:02:25s\n",
      "epoch 155| loss: 38855795.625|  0:02:25s\n",
      "epoch 156| loss: 37686462.83333|  0:02:26s\n",
      "epoch 157| loss: 34783859.54167|  0:02:26s\n",
      "epoch 158| loss: 38849322.77083|  0:02:27s\n",
      "epoch 159| loss: 35012038.0|  0:02:27s\n",
      "epoch 160| loss: 22557891.45833|  0:02:28s\n",
      "epoch 161| loss: 35626718.29167|  0:02:28s\n",
      "epoch 162| loss: 36007989.8125|  0:02:28s\n",
      "epoch 163| loss: 32277890.66667|  0:02:29s\n",
      "epoch 164| loss: 32101245.52083|  0:02:29s\n",
      "epoch 165| loss: 37493866.64583|  0:02:30s\n",
      "epoch 166| loss: 31979216.75|  0:02:30s\n",
      "epoch 167| loss: 31808122.20833|  0:02:31s\n",
      "epoch 168| loss: 33068632.375|  0:02:31s\n",
      "epoch 169| loss: 30527601.47917|  0:02:31s\n",
      "epoch 170| loss: 30883079.29167|  0:02:32s\n",
      "epoch 171| loss: 31931128.45833|  0:02:32s\n",
      "epoch 172| loss: 30443313.6875|  0:02:33s\n",
      "epoch 173| loss: 33983318.16667|  0:02:33s\n",
      "epoch 174| loss: 22149977.41667|  0:02:34s\n",
      "epoch 175| loss: 27896093.77083|  0:02:34s\n",
      "epoch 176| loss: 31137517.27083|  0:02:35s\n",
      "epoch 177| loss: 29164525.02083|  0:02:36s\n",
      "epoch 178| loss: 33715380.5|  0:02:38s\n",
      "epoch 179| loss: 32037035.09375|  0:02:40s\n",
      "epoch 180| loss: 34641106.8125|  0:02:42s\n",
      "epoch 181| loss: 36863791.97917|  0:02:43s\n",
      "epoch 182| loss: 32556034.83333|  0:02:45s\n",
      "epoch 183| loss: 38734311.26042|  0:02:47s\n",
      "epoch 184| loss: 31820360.45833|  0:02:48s\n",
      "epoch 185| loss: 30931708.58333|  0:02:50s\n",
      "epoch 186| loss: 22285069.02083|  0:02:52s\n",
      "epoch 187| loss: 30752130.45833|  0:02:54s\n",
      "epoch 188| loss: 20695032.41667|  0:02:55s\n",
      "epoch 189| loss: 31610945.10417|  0:02:57s\n",
      "epoch 190| loss: 28130656.13542|  0:02:59s\n",
      "epoch 191| loss: 38829606.875|  0:03:00s\n",
      "epoch 192| loss: 30844018.10417|  0:03:02s\n",
      "epoch 193| loss: 29982826.66667|  0:03:04s\n",
      "epoch 194| loss: 30610807.1875|  0:03:05s\n",
      "epoch 195| loss: 37093063.125|  0:03:07s\n",
      "epoch 196| loss: 34551352.14583|  0:03:09s\n",
      "epoch 197| loss: 28471069.29167|  0:03:11s\n",
      "epoch 198| loss: 34780452.875|  0:03:12s\n",
      "epoch 199| loss: 28224798.875|  0:03:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 111080053.0|  0:00:01s\n",
      "epoch 1  | loss: 122103835.33333|  0:00:03s\n",
      "epoch 2  | loss: 122174652.33333|  0:00:04s\n",
      "epoch 3  | loss: 113593018.0|  0:00:06s\n",
      "epoch 4  | loss: 121201652.66667|  0:00:08s\n",
      "epoch 5  | loss: 120173610.66667|  0:00:09s\n",
      "epoch 6  | loss: 120473786.66667|  0:00:11s\n",
      "epoch 7  | loss: 119894074.66667|  0:00:13s\n",
      "epoch 8  | loss: 119877984.33333|  0:00:15s\n",
      "epoch 9  | loss: 119131366.66667|  0:00:17s\n",
      "epoch 10 | loss: 119343056.66667|  0:00:18s\n",
      "epoch 11 | loss: 118425522.33333|  0:00:20s\n",
      "epoch 12 | loss: 117926230.66667|  0:00:22s\n",
      "epoch 13 | loss: 117431614.33333|  0:00:24s\n",
      "epoch 14 | loss: 116063414.33333|  0:00:25s\n",
      "epoch 15 | loss: 115345660.66667|  0:00:27s\n",
      "epoch 16 | loss: 115275388.66667|  0:00:28s\n",
      "epoch 17 | loss: 108698701.0|  0:00:30s\n",
      "epoch 18 | loss: 112792460.66667|  0:00:32s\n",
      "epoch 19 | loss: 112052684.66667|  0:00:34s\n",
      "epoch 20 | loss: 111257864.33333|  0:00:35s\n",
      "epoch 21 | loss: 99430903.0|  0:00:37s\n",
      "epoch 22 | loss: 107857362.33333|  0:00:38s\n",
      "epoch 23 | loss: 107200044.0|  0:00:40s\n",
      "epoch 24 | loss: 106839139.33333|  0:00:41s\n",
      "epoch 25 | loss: 104520331.66667|  0:00:43s\n",
      "epoch 26 | loss: 104462360.5|  0:00:44s\n",
      "epoch 27 | loss: 103083800.66667|  0:00:46s\n",
      "epoch 28 | loss: 100531332.0|  0:00:47s\n",
      "epoch 29 | loss: 99927974.66667|  0:00:49s\n",
      "epoch 30 | loss: 93485696.83333|  0:00:51s\n",
      "epoch 31 | loss: 96772013.5|  0:00:52s\n",
      "epoch 32 | loss: 95720550.16667|  0:00:54s\n",
      "epoch 33 | loss: 94073807.83333|  0:00:56s\n",
      "epoch 34 | loss: 92439557.83333|  0:00:57s\n",
      "epoch 35 | loss: 91008934.16667|  0:00:59s\n",
      "epoch 36 | loss: 90340903.0|  0:01:01s\n",
      "epoch 37 | loss: 88548440.66667|  0:01:03s\n",
      "epoch 38 | loss: 87190208.5|  0:01:04s\n",
      "epoch 39 | loss: 86028376.83333|  0:01:06s\n",
      "epoch 40 | loss: 84658207.83333|  0:01:08s\n",
      "epoch 41 | loss: 74510642.0|  0:01:09s\n",
      "epoch 42 | loss: 81940429.0|  0:01:11s\n",
      "epoch 43 | loss: 63726224.16667|  0:01:13s\n",
      "epoch 44 | loss: 59755318.58333|  0:01:14s\n",
      "epoch 45 | loss: 78443872.5|  0:01:16s\n",
      "epoch 46 | loss: 76585242.41667|  0:01:18s\n",
      "epoch 47 | loss: 75141620.5|  0:01:19s\n",
      "epoch 48 | loss: 74113129.75|  0:01:21s\n",
      "epoch 49 | loss: 46070430.0|  0:01:23s\n",
      "epoch 50 | loss: 71539877.58333|  0:01:25s\n",
      "epoch 51 | loss: 64677019.0|  0:01:27s\n",
      "epoch 52 | loss: 68314636.95833|  0:01:28s\n",
      "epoch 53 | loss: 67335844.41667|  0:01:30s\n",
      "epoch 54 | loss: 65888569.75|  0:01:32s\n",
      "epoch 55 | loss: 66987640.25|  0:01:33s\n",
      "epoch 56 | loss: 65280340.08333|  0:01:35s\n",
      "epoch 57 | loss: 63854074.875|  0:01:37s\n",
      "epoch 58 | loss: 62862649.08333|  0:01:39s\n",
      "epoch 59 | loss: 56685003.95833|  0:01:40s\n",
      "epoch 60 | loss: 61011458.58333|  0:01:42s\n",
      "epoch 61 | loss: 59756906.625|  0:01:44s\n",
      "epoch 62 | loss: 60158732.91667|  0:01:46s\n",
      "epoch 63 | loss: 57895379.08333|  0:01:47s\n",
      "epoch 64 | loss: 57911710.625|  0:01:49s\n",
      "epoch 65 | loss: 58929063.41667|  0:01:51s\n",
      "epoch 66 | loss: 57553381.875|  0:01:53s\n",
      "epoch 67 | loss: 56669167.08333|  0:01:54s\n",
      "epoch 68 | loss: 56973287.29167|  0:01:56s\n",
      "epoch 69 | loss: 48634052.66667|  0:01:57s\n",
      "epoch 70 | loss: 57515471.83333|  0:01:59s\n",
      "epoch 71 | loss: 54929241.58333|  0:02:01s\n",
      "epoch 72 | loss: 53660529.125|  0:02:02s\n",
      "epoch 73 | loss: 55354816.04167|  0:02:04s\n",
      "epoch 74 | loss: 53891022.02083|  0:02:06s\n",
      "epoch 75 | loss: 36134572.25|  0:02:08s\n",
      "epoch 76 | loss: 53869003.39583|  0:02:09s\n",
      "epoch 77 | loss: 52018881.60417|  0:02:11s\n",
      "epoch 78 | loss: 52018767.29167|  0:02:13s\n",
      "epoch 79 | loss: 50990894.16667|  0:02:15s\n",
      "epoch 80 | loss: 52681224.66667|  0:02:16s\n",
      "epoch 81 | loss: 53721283.1875|  0:02:18s\n",
      "epoch 82 | loss: 51283157.20833|  0:02:20s\n",
      "epoch 83 | loss: 50064214.85417|  0:02:21s\n",
      "epoch 84 | loss: 50485490.875|  0:02:23s\n",
      "epoch 85 | loss: 50906592.70833|  0:02:25s\n",
      "epoch 86 | loss: 49054159.10417|  0:02:26s\n",
      "epoch 87 | loss: 48222644.70833|  0:02:28s\n",
      "epoch 88 | loss: 51820111.75|  0:02:30s\n",
      "epoch 89 | loss: 50755458.70833|  0:02:30s\n",
      "epoch 90 | loss: 50572572.41667|  0:02:32s\n",
      "epoch 91 | loss: 51215144.72917|  0:02:34s\n",
      "epoch 92 | loss: 49957562.39583|  0:02:35s\n",
      "epoch 93 | loss: 43984415.625|  0:02:37s\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tabnet\"\n",
    "oof_preds = get_oof_preds(\n",
    "                            folds,\n",
    "                            X_train_tab, y_train_tab,\n",
    "                            model_pred_name = model_name,\n",
    "                            model_gen_name = model_gen,\n",
    "                            n_samples_ratio = n_samples_ratio,\n",
    "                            doAugment=False\n",
    ")\n",
    "\n",
    "\n",
    "meta_model = train_meta_model(X_tr = oof_preds, y_tr = y_train_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeab69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated an existing row\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "oof_models = save_data.load_oof_models(\n",
    "    dataset_name=dataset_name,\n",
    "    model_pred=model_name,\n",
    "    model_gen=model_gen\n",
    ")\n",
    "\n",
    "display(len(oof_models))\n",
    "\n",
    "mean_prevs = np.zeros(len(y_test)).reshape(-1,1)\n",
    "\n",
    "for oof_model in oof_models:\n",
    "    mean_prevs += (1/n_splits)*test_pred_model(oof_model, X_test_tab)\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(mean_prevs)\n",
    "\n",
    "\n",
    "models_tracker.add_metrics(\n",
    "                dataset=dataset_name, \n",
    "                y_true=y_test_tab.ravel(), \n",
    "                y_pred=final_preds, \n",
    "                pred_name=model_name,\n",
    "                gen_name=model_gen, \n",
    "                ratio=n_samples_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c6c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Generative Model</th>\n",
       "      <th>Predicative Model</th>\n",
       "      <th>Synthetic Data Ratio</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275269</td>\n",
       "      <td>0.185445</td>\n",
       "      <td>0.430633</td>\n",
       "      <td>0.863098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275137</td>\n",
       "      <td>0.192930</td>\n",
       "      <td>0.439238</td>\n",
       "      <td>0.857572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.283502</td>\n",
       "      <td>0.198185</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.853692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.289142</td>\n",
       "      <td>0.199284</td>\n",
       "      <td>0.446412</td>\n",
       "      <td>0.852881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285197</td>\n",
       "      <td>0.199349</td>\n",
       "      <td>0.446485</td>\n",
       "      <td>0.852833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.290652</td>\n",
       "      <td>0.201168</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.851490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291499</td>\n",
       "      <td>0.201379</td>\n",
       "      <td>0.448753</td>\n",
       "      <td>0.851334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288232</td>\n",
       "      <td>0.203485</td>\n",
       "      <td>0.451093</td>\n",
       "      <td>0.849779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.286947</td>\n",
       "      <td>0.203705</td>\n",
       "      <td>0.451337</td>\n",
       "      <td>0.849617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.292119</td>\n",
       "      <td>0.204821</td>\n",
       "      <td>0.452571</td>\n",
       "      <td>0.848794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.293255</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.453436</td>\n",
       "      <td>0.848215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291875</td>\n",
       "      <td>0.209690</td>\n",
       "      <td>0.457919</td>\n",
       "      <td>0.845199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.301633</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.842190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.314072</td>\n",
       "      <td>0.225565</td>\n",
       "      <td>0.474936</td>\n",
       "      <td>0.833480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.317982</td>\n",
       "      <td>0.230561</td>\n",
       "      <td>0.480167</td>\n",
       "      <td>0.829791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322342</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>0.488217</td>\n",
       "      <td>0.824037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>0.239741</td>\n",
       "      <td>0.489634</td>\n",
       "      <td>0.823014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.331311</td>\n",
       "      <td>0.249159</td>\n",
       "      <td>0.499158</td>\n",
       "      <td>0.816061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.398988</td>\n",
       "      <td>0.355158</td>\n",
       "      <td>0.595951</td>\n",
       "      <td>0.737809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.400457</td>\n",
       "      <td>0.357177</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.736318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.365938</td>\n",
       "      <td>0.604928</td>\n",
       "      <td>0.729851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.406634</td>\n",
       "      <td>0.374362</td>\n",
       "      <td>0.611852</td>\n",
       "      <td>0.723631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.418725</td>\n",
       "      <td>0.375949</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.722460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.416784</td>\n",
       "      <td>0.377534</td>\n",
       "      <td>0.614438</td>\n",
       "      <td>0.721290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.417490</td>\n",
       "      <td>0.381695</td>\n",
       "      <td>0.617815</td>\n",
       "      <td>0.718218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset Generative Model Predicative Model  \\\n",
       "2   California Housing                -           xgboost   \n",
       "3   California Housing                -          lightgbm   \n",
       "19  California Housing              gan          lightgbm   \n",
       "18  California Housing              gan           xgboost   \n",
       "7   California Housing        copulagan          lightgbm   \n",
       "22  California Housing              vae           xgboost   \n",
       "6   California Housing        copulagan           xgboost   \n",
       "15  California Housing            ctgan          lightgbm   \n",
       "23  California Housing              vae          lightgbm   \n",
       "14  California Housing            ctgan           xgboost   \n",
       "10  California Housing             tvae           xgboost   \n",
       "11  California Housing             tvae          lightgbm   \n",
       "1   California Housing                -          catboost   \n",
       "5   California Housing        copulagan          catboost   \n",
       "13  California Housing            ctgan          catboost   \n",
       "9   California Housing             tvae          catboost   \n",
       "17  California Housing              gan          catboost   \n",
       "21  California Housing              vae          catboost   \n",
       "20  California Housing              vae            tabnet   \n",
       "24  California Housing             ddpm            tabnet   \n",
       "0   California Housing                -            tabnet   \n",
       "16  California Housing              gan            tabnet   \n",
       "12  California Housing            ctgan            tabnet   \n",
       "4   California Housing        copulagan            tabnet   \n",
       "8   California Housing             tvae            tabnet   \n",
       "\n",
       "    Synthetic Data Ratio       MAE       MSE      RMSE        R2  \n",
       "2                   0.00  0.275269  0.185445  0.430633  0.863098  \n",
       "3                   0.00  0.275137  0.192930  0.439238  0.857572  \n",
       "19                  0.25  0.283502  0.198185  0.445180  0.853692  \n",
       "18                  0.25  0.289142  0.199284  0.446412  0.852881  \n",
       "7                   0.25  0.285197  0.199349  0.446485  0.852833  \n",
       "22                  0.25  0.290652  0.201168  0.448518  0.851490  \n",
       "6                   0.25  0.291499  0.201379  0.448753  0.851334  \n",
       "15                  0.25  0.288232  0.203485  0.451093  0.849779  \n",
       "23                  0.25  0.286947  0.203705  0.451337  0.849617  \n",
       "14                  0.25  0.292119  0.204821  0.452571  0.848794  \n",
       "10                  0.25  0.293255  0.205604  0.453436  0.848215  \n",
       "11                  0.25  0.291875  0.209690  0.457919  0.845199  \n",
       "1                   0.00  0.301633  0.213766  0.462348  0.842190  \n",
       "5                   0.25  0.314072  0.225565  0.474936  0.833480  \n",
       "13                  0.25  0.317982  0.230561  0.480167  0.829791  \n",
       "9                   0.25  0.322342  0.238356  0.488217  0.824037  \n",
       "17                  0.25  0.322549  0.239741  0.489634  0.823014  \n",
       "21                  0.25  0.331311  0.249159  0.499158  0.816061  \n",
       "20                  0.25  0.398988  0.355158  0.595951  0.737809  \n",
       "24                  0.25  0.400457  0.357177  0.597643  0.736318  \n",
       "0                   0.00  0.408754  0.365938  0.604928  0.729851  \n",
       "16                  0.25  0.406634  0.374362  0.611852  0.723631  \n",
       "12                  0.25  0.418725  0.375949  0.613147  0.722460  \n",
       "4                   0.25  0.416784  0.377534  0.614438  0.721290  \n",
       "8                   0.25  0.417490  0.381695  0.617815  0.718218  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(models_tracker.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e59086c",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "oof_preds = get_oof_preds(\n",
    "                            folds,\n",
    "                            X_train_trees, y_train.to_numpy(),\n",
    "                            model_pred_name = model_name,\n",
    "                            model_gen_name = model_gen,\n",
    "                            n_samples_ratio = n_samples_ratio\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta_model = train_meta_model(X_tr = oof_preds, y_tr = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d554c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model metrics added\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "oof_models = save_data.load_oof_models(\n",
    "    dataset_name=dataset_name,\n",
    "    model_pred=model_name,\n",
    "    model_gen=model_gen\n",
    ")\n",
    "\n",
    "display(len(oof_models))\n",
    "\n",
    "mean_prevs = np.zeros(len(y_test)).reshape(-1,1)\n",
    "\n",
    "for oof_model in oof_models:\n",
    "    mean_prevs += (1/n_splits)*test_pred_model(oof_model, X_test_trees)\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(mean_prevs)\n",
    "\n",
    "\n",
    "models_tracker.add_metrics(\n",
    "                dataset=dataset_name, \n",
    "                y_true=y_test.to_numpy().ravel(), \n",
    "                y_pred=final_preds, \n",
    "                pred_name=model_name,\n",
    "                gen_name=model_gen, \n",
    "                ratio=n_samples_ratio\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeec3ee",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n",
      "Existe o Fold\n",
      "So iguais\n",
      "Existe o Fold\n",
      "So iguais\n",
      "Existe o Fold\n",
      "So iguais\n",
      "Existe o Fold\n",
      "So iguais\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "oof_preds = get_oof_preds(\n",
    "                            folds,\n",
    "                            X_train_trees, y_train.to_numpy(),\n",
    "                            model_pred_name = model_name,\n",
    "                            model_gen_name = model_gen,\n",
    "                            n_samples_ratio = n_samples_ratio\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta_model = train_meta_model(X_tr = oof_preds, y_tr = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb56151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model metrics added\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "oof_models = save_data.load_oof_models(\n",
    "    dataset_name=dataset_name,\n",
    "    model_pred=model_name,\n",
    "    model_gen=model_gen\n",
    ")\n",
    "\n",
    "display(len(oof_models))\n",
    "\n",
    "mean_prevs = np.zeros(len(y_test)).reshape(-1,1)\n",
    "\n",
    "for oof_model in oof_models:\n",
    "    mean_prevs += (1/n_splits)*test_pred_model(oof_model, X_test_trees)\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(mean_prevs)\n",
    "\n",
    "\n",
    "models_tracker.add_metrics(\n",
    "                dataset=dataset_name, \n",
    "                y_true=y_test.to_numpy().ravel(), \n",
    "                y_pred=final_preds, \n",
    "                pred_name=model_name,\n",
    "                gen_name=model_gen, \n",
    "                ratio=n_samples_ratio\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2f2ae9",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a475ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe o Fold\n",
      "So iguais\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lightgbm\"\n",
    "\n",
    "oof_preds = get_oof_preds(\n",
    "                            folds,\n",
    "                            X_train_trees, y_train.to_numpy(),\n",
    "                            model_pred_name = model_name,\n",
    "                            model_gen_name = model_gen,\n",
    "                            n_samples_ratio = n_samples_ratio\n",
    ")\n",
    "\n",
    "\n",
    "meta_model = train_meta_model(X_tr = oof_preds, y_tr = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1865863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model metrics added\n",
      "File Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Utilizador\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "oof_models = save_data.load_oof_models(\n",
    "    dataset_name=dataset_name,\n",
    "    model_pred=model_name,\n",
    "    model_gen=model_gen\n",
    ")\n",
    "\n",
    "display(len(oof_models))\n",
    "\n",
    "mean_prevs = np.zeros(len(y_test)).reshape(-1,1)\n",
    "\n",
    "for oof_model in oof_models:\n",
    "    oof_model.feature_name_\n",
    "    mean_prevs += (1/n_splits)*test_pred_model(oof_model, X_test_trees)\n",
    "\n",
    "\n",
    "final_preds = meta_model.predict(mean_prevs)\n",
    "\n",
    "\n",
    "models_tracker.add_metrics(\n",
    "                dataset=dataset_name, \n",
    "                y_true=y_test.to_numpy().ravel(), \n",
    "                y_pred=final_preds, \n",
    "                pred_name=model_name,\n",
    "                gen_name=model_gen, \n",
    "                ratio=n_samples_ratio\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ccdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Generative Model</th>\n",
       "      <th>Predicative Model</th>\n",
       "      <th>Synthetic Data Ratio</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275269</td>\n",
       "      <td>0.185445</td>\n",
       "      <td>0.430633</td>\n",
       "      <td>0.863098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.275137</td>\n",
       "      <td>0.192930</td>\n",
       "      <td>0.439238</td>\n",
       "      <td>0.857572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.286503</td>\n",
       "      <td>0.198146</td>\n",
       "      <td>0.445136</td>\n",
       "      <td>0.853721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.283502</td>\n",
       "      <td>0.198185</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.853692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.282377</td>\n",
       "      <td>0.198832</td>\n",
       "      <td>0.445905</td>\n",
       "      <td>0.853215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.289142</td>\n",
       "      <td>0.199284</td>\n",
       "      <td>0.446412</td>\n",
       "      <td>0.852881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.285197</td>\n",
       "      <td>0.199349</td>\n",
       "      <td>0.446485</td>\n",
       "      <td>0.852833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.290652</td>\n",
       "      <td>0.201168</td>\n",
       "      <td>0.448518</td>\n",
       "      <td>0.851490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291499</td>\n",
       "      <td>0.201379</td>\n",
       "      <td>0.448753</td>\n",
       "      <td>0.851334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.288232</td>\n",
       "      <td>0.203485</td>\n",
       "      <td>0.451093</td>\n",
       "      <td>0.849779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.286947</td>\n",
       "      <td>0.203705</td>\n",
       "      <td>0.451337</td>\n",
       "      <td>0.849617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.292119</td>\n",
       "      <td>0.204821</td>\n",
       "      <td>0.452571</td>\n",
       "      <td>0.848794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.293255</td>\n",
       "      <td>0.205604</td>\n",
       "      <td>0.453436</td>\n",
       "      <td>0.848215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.291875</td>\n",
       "      <td>0.209690</td>\n",
       "      <td>0.457919</td>\n",
       "      <td>0.845199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.301633</td>\n",
       "      <td>0.213766</td>\n",
       "      <td>0.462348</td>\n",
       "      <td>0.842190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.310913</td>\n",
       "      <td>0.223940</td>\n",
       "      <td>0.473223</td>\n",
       "      <td>0.834679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.314072</td>\n",
       "      <td>0.225565</td>\n",
       "      <td>0.474936</td>\n",
       "      <td>0.833480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.317982</td>\n",
       "      <td>0.230561</td>\n",
       "      <td>0.480167</td>\n",
       "      <td>0.829791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322342</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>0.488217</td>\n",
       "      <td>0.824037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.322549</td>\n",
       "      <td>0.239741</td>\n",
       "      <td>0.489634</td>\n",
       "      <td>0.823014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>catboost</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.331311</td>\n",
       "      <td>0.249159</td>\n",
       "      <td>0.499158</td>\n",
       "      <td>0.816061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>vae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.398988</td>\n",
       "      <td>0.355158</td>\n",
       "      <td>0.595951</td>\n",
       "      <td>0.737809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ddpm</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.400457</td>\n",
       "      <td>0.357177</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.736318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>-</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.365938</td>\n",
       "      <td>0.604928</td>\n",
       "      <td>0.729851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>gan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.406634</td>\n",
       "      <td>0.374362</td>\n",
       "      <td>0.611852</td>\n",
       "      <td>0.723631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>ctgan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.418725</td>\n",
       "      <td>0.375949</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.722460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>copulagan</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.416784</td>\n",
       "      <td>0.377534</td>\n",
       "      <td>0.614438</td>\n",
       "      <td>0.721290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>California Housing</td>\n",
       "      <td>tvae</td>\n",
       "      <td>tabnet</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.417490</td>\n",
       "      <td>0.381695</td>\n",
       "      <td>0.617815</td>\n",
       "      <td>0.718218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset Generative Model Predicative Model  \\\n",
       "2   California Housing                -           xgboost   \n",
       "3   California Housing                -          lightgbm   \n",
       "26  California Housing             ddpm           xgboost   \n",
       "19  California Housing              gan          lightgbm   \n",
       "27  California Housing             ddpm          lightgbm   \n",
       "18  California Housing              gan           xgboost   \n",
       "7   California Housing        copulagan          lightgbm   \n",
       "22  California Housing              vae           xgboost   \n",
       "6   California Housing        copulagan           xgboost   \n",
       "15  California Housing            ctgan          lightgbm   \n",
       "23  California Housing              vae          lightgbm   \n",
       "14  California Housing            ctgan           xgboost   \n",
       "10  California Housing             tvae           xgboost   \n",
       "11  California Housing             tvae          lightgbm   \n",
       "1   California Housing                -          catboost   \n",
       "25  California Housing             ddpm          catboost   \n",
       "5   California Housing        copulagan          catboost   \n",
       "13  California Housing            ctgan          catboost   \n",
       "9   California Housing             tvae          catboost   \n",
       "17  California Housing              gan          catboost   \n",
       "21  California Housing              vae          catboost   \n",
       "20  California Housing              vae            tabnet   \n",
       "24  California Housing             ddpm            tabnet   \n",
       "0   California Housing                -            tabnet   \n",
       "16  California Housing              gan            tabnet   \n",
       "12  California Housing            ctgan            tabnet   \n",
       "4   California Housing        copulagan            tabnet   \n",
       "8   California Housing             tvae            tabnet   \n",
       "\n",
       "    Synthetic Data Ratio       MAE       MSE      RMSE        R2  \n",
       "2                   0.00  0.275269  0.185445  0.430633  0.863098  \n",
       "3                   0.00  0.275137  0.192930  0.439238  0.857572  \n",
       "26                  0.25  0.286503  0.198146  0.445136  0.853721  \n",
       "19                  0.25  0.283502  0.198185  0.445180  0.853692  \n",
       "27                  0.25  0.282377  0.198832  0.445905  0.853215  \n",
       "18                  0.25  0.289142  0.199284  0.446412  0.852881  \n",
       "7                   0.25  0.285197  0.199349  0.446485  0.852833  \n",
       "22                  0.25  0.290652  0.201168  0.448518  0.851490  \n",
       "6                   0.25  0.291499  0.201379  0.448753  0.851334  \n",
       "15                  0.25  0.288232  0.203485  0.451093  0.849779  \n",
       "23                  0.25  0.286947  0.203705  0.451337  0.849617  \n",
       "14                  0.25  0.292119  0.204821  0.452571  0.848794  \n",
       "10                  0.25  0.293255  0.205604  0.453436  0.848215  \n",
       "11                  0.25  0.291875  0.209690  0.457919  0.845199  \n",
       "1                   0.00  0.301633  0.213766  0.462348  0.842190  \n",
       "25                  0.25  0.310913  0.223940  0.473223  0.834679  \n",
       "5                   0.25  0.314072  0.225565  0.474936  0.833480  \n",
       "13                  0.25  0.317982  0.230561  0.480167  0.829791  \n",
       "9                   0.25  0.322342  0.238356  0.488217  0.824037  \n",
       "17                  0.25  0.322549  0.239741  0.489634  0.823014  \n",
       "21                  0.25  0.331311  0.249159  0.499158  0.816061  \n",
       "20                  0.25  0.398988  0.355158  0.595951  0.737809  \n",
       "24                  0.25  0.400457  0.357177  0.597643  0.736318  \n",
       "0                   0.00  0.408754  0.365938  0.604928  0.729851  \n",
       "16                  0.25  0.406634  0.374362  0.611852  0.723631  \n",
       "12                  0.25  0.418725  0.375949  0.613147  0.722460  \n",
       "4                   0.25  0.416784  0.377534  0.614438  0.721290  \n",
       "8                   0.25  0.417490  0.381695  0.617815  0.718218  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(models_tracker.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00b3f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<string>:55\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#def load_synthetic_fold(model_gen, dataset, ratio, fold_idx, base_path=\"synthetic_data\", isTrees = True):\n",
    "\"\"\"\n",
    "def retrain_oof_models(model_name):\n",
    "\n",
    "    oof_models_list = []\n",
    "    oof_preds = np.zeros(len(y_train))\n",
    "\n",
    "    for fold_idx in range(n_splits):\n",
    "        X_syn, y_syn, tr_idx, val_idx = save_data.load_synthetic_fold(\n",
    "                                                        model_gen=model_gen,\n",
    "                                                        dataset=dataset_name,\n",
    "                                                        ratio=n_samples_ratio,\n",
    "                                                        fold_idx=fold_idx,\n",
    "                                                        isTrees=True\n",
    "                                                    )\n",
    "        \n",
    "        X_tr, y_tr = X_train_trees[tr_idx], y_train[tr_idx]\n",
    "        X_val, y_val = X_train_trees[val_idx], y_train[val_idx]\n",
    "        \n",
    "        X_tr_aug = np.vstack([X_tr, X_syn])\n",
    "        y_tr_aug = np.concatenate([y_tr.ravel(), y_syn])\n",
    "\n",
    "\n",
    "        trained_model = train_pred_model(\n",
    "                            model_name=model_name,\n",
    "                            X_tr=X_tr_aug,\n",
    "                            y_tr=y_tr_aug,\n",
    "                        )\n",
    "\n",
    "\n",
    "        oof_models_list.append(trained_model)\n",
    "        preds_on_fold = test_pred_model(trained_model, X_val)\n",
    "\n",
    "        oof_preds[val_idx.reshape(-1,1)] = preds_on_fold\n",
    "\n",
    "    \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        save_data.save_oof_models(\n",
    "            dataset_name= dataset_name,\n",
    "            models_list = oof_models_list,\n",
    "            model_pred = model_pred_name,\n",
    "            model_gen = model_gen\n",
    "        )\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return oof_preds, oof_models_list\n",
    "    \n",
    "if model_gen != \"\":\n",
    "    oof_preds, oof_models_list = retrain_oof_models(model_name)\n",
    "\n",
    "    meta_model = train_meta_model(X_tr = oof_preds, y_tr = y_train)\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    oof_models = save_data.load_oof_models(\n",
    "        dataset_name=dataset_name,\n",
    "        model_pred=model_name,\n",
    "        model_gen=model_gen\n",
    "    )\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    display(len(oof_models)\n",
    "    \n",
    "    )\n",
    "\n",
    "    mean_prevs = np.zeros(len(y_test)).reshape(-1,1)\n",
    "\n",
    "    for oof_model in oof_models:\n",
    "        oof_model.feature_name_\n",
    "        mean_prevs += (1/n_splits)*test_pred_model(oof_model, X_test_trees)\n",
    "\n",
    "\n",
    "    final_preds = meta_model.predict(mean_prevs)\n",
    "\n",
    "\n",
    "    models_tracker.add_metrics(\n",
    "                    dataset=dataset_name, \n",
    "                    y_true=y_test.to_numpy().ravel(), \n",
    "                    y_pred=final_preds, \n",
    "                    pred_name=model_name,\n",
    "                    gen_name=\"test\", \n",
    "                    ratio=n_samples_ratio\n",
    "                )\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
